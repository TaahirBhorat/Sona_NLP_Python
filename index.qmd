---
title: "SONA_NLP_Python"
---

# Introduction

This paper critically analyzes the State of the Nation Address (SONA) speeches delivered by various South African presidents from 1994 to 2023. The primary objective is to categorize each president based on single sentences extracted from their respective SONA speeches. The study unfolds in XXX main sections.

Initially, a concise literature review is presented, with emphasis on the domain of Natural Language Processing (NLP), particularly focusing on classification tasks within NLP. This review lays the groundwork for the methodologies and approaches applied in later sections of the paper.

Subsequent sections offer an in-depth exploration and meticulous cleaning of the data utilized in the study. The exploration phase scrutinizes the dataset's balance and analyzes the vocabulary used, both overall and by each specific president. These preliminary steps are crucial for ensuring the integrity and reliability of the study's findings.

The paper then transitions to a detailed exposition of the methodologies employed in the study. The methods section elucidates the three feature extraction tools deployed: Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Word Tokenization. Additionally, it describes the XXX predictive models applied, namely Gradient Boosted Trees, Feed Forward Neural Networks, and Support Vector Machines. Each tool and model is presented with a rationale for its inclusion and an explanation of its contribution to the study's objectives.

Following the methods section, the paper presents and succinctly discusses the study's results. This section provides an initial interpretation of the findings, preparing the ground for the more in-depth analysis that follows.

In the penultimate section, a comprehensive discussion of the results is provided. This discussion delves into the insights gleaned from the findings, offering detailed interpretations and drawing connections with the literature reviewed earlier. This section aims not only to shed light on the study's findings but also to locate these within the broader academic discourse on the subject.

Finally, the paper concludes with a reflective overview of the study as a whole. This concluding section evaluates the study's successes and limitations, reflects on its contributions to the field, and suggests avenues for future research and exploration. Through this reflective lens, the paper not only summarizes its findings but also invites further scholarly engagement with the questions and challenges raised during the study.

# Literature Review

# Methods

A variety of methods were employed throughout this project, though they can be broken down into three main uses: Feature extraction, modelling, and model evaluation 

## Feature Extraction

### Bag of Words (BoW)
Bag of Words (BoW) simplifies the representation of text into a set of words and their corresponding frequency in a document. Each document is represented as a vector within a multidimensional space where each dimension corresponds to a word from the corpus. Mathematically, given a text \( T \) consisting of sentences \( s_1, s_2, \ldots, s_n \), each sentence is represented as a vector \( v_i \) where each dimension corresponds to a unique word, and the value at each dimension \( j \) is the frequency count of word \( w_j \) in sentence \( s_i \).

\[ v_i = [ \text{count}(w_1), \text{count}(w_2), \ldots, \text{count}(w_m) ] \]
Here each sentence was considereed a doccument so rows in BoW representation were inidiividual sentences

### Term Frequency-Inverse Document Frequency (TF-IDF)
TF-IDF emphasizes important words which are frequent in a document but not across documents. Term Frequency (TF) represents the frequency of a word in a document, and Inverse Document Frequency (IDF) diminishes the weight of words that occur frequently in the corpus.

\[ \text{TF}(w, d) = \frac{\text{Number of times word } w \text{ appears in document } d}{\text{Total number of words in document } d} \]
\[ \text{IDF}(w, D) = \log\left(\frac{\text{Total number of documents in corpus } D}{\text{Number of documents containing word } w}\right) \]
\[ \text{TF-IDF}(w, d, D) = \text{TF}(w, d) \times \text{IDF}(w, D) \]

### Word Embeddings
Word2Vec algorithm learns the context of words in the text to generate word embeddings. It constructs a high-dimensional space where similar words are placed closer to each other. Sentence embeddings are calculated as the mean of the word embeddings of the words constituting the sentence.

## Modeling

### Gradient Boosted Trees (CatBoost)
CatBoost is a gradient boosting algorithm that is based on decision trees. It successively refines its predictions over multiple rounds, each of which consists of creating and adding a new tree that corrects the errors of the combined ensemble of existing trees. The objective is to minimize the loss function by adding weak learners using a gradient descent-like procedure().

### Support Vector Machines (SVM)
Support Vector Machines (SVM) operate in a multidimensional space, represented by the features of the data points. The SVM constructs a hyperplane in this space to separate the different classes. The optimal hyperplane is the one that maximises the margin, which is the distance between the hyperplane and the nearest data point of each class. SVMs can use different types of kernels (linear, polynomial, radial basis function, etc.) to transform the input space in such a way that separation is easier.

### Feed Forward Neural Networks
A Feed Forward Neural Network consists of an input layer, hidden layers, and an output layer. Each layer contains neurons, and adjacent layers are fully connected through weighted edges. During training, the weights are adjusted using backpropagation and a gradient descent or variant algorithm to minimize the error between the network's prediction and the actual target values.

## Model Evaluation

Model performance was evaluated based on accuracy:

\[ \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions Made}} \]

Precision, recall, and F1-score for each class were also considered:

\[ \text{Precision} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Positive}} \]
\[ \text{Recall} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Negative}} \]
\[ \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} \]

Confusion matrices were generated for each model, offering a clear visualization of the performance of the classification algorithm. It illustrates the correct predictions and the errors made, making it easier to interpret the results, especially in multi-class classification problems. 

For the SVM and Neural Network models, grid searches were performed where necessary to select optimal hyperparameters for performance enhancement. Grid search meticulously works through multiple combinations of hyperparameter tunes, cross-validating as it goes to determine which tune gives the best performance based on a predefined metric, usually accuracy. Each model's training and evaluation were conducted with different feature extraction techniques, providing a thorough understanding of how each feature extraction method impacts the modelâ€™s performance.

```{python}
pip install nltk requests matplotlib seaborn sklearn scikit-learn wordcloud catboost tensorflow gensim
```

```{python}
# General imports
import os
import pandas as pd
import re
import numpy as np

# NLTK imports
import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

# Visualization imports
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS

# Preprocessing imports
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

# Model selection imports
from sklearn.model_selection import train_test_split, GridSearchCV

# Machine learning model imports
from sklearn.svm import SVC
from catboost import CatBoostClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Word embedding imports
from gensim.models import Word2Vec

# Metrics import
from sklearn.metrics import classification_report, accuracy_score
```


```{python}
folder_path = 'speeches'  # Ensure this is your correct folder path
files = os.listdir(folder_path)
files = sorted([file for file in files if os.path.isfile(os.path.join(folder_path, file)) and file.endswith('.txt')])

president_names = []

# Updated regex pattern to handle more cases
pattern = r'_(.+?)\.txt'  # Non-greedy match to get the president name

for file in files:
    match = re.search(pattern, file)
    if match:
        president_name = match.group(1)
        # Remove the "_2" suffix from the president names here
        cleaned_president_name = president_name.replace('_2', '')
        president_names.append(cleaned_president_name)
    else:
        print(f"Warning: No match found in filename: {file}")
        president_names.append('Unknown')  # Placeholder for missing names

# Check the lengths of files and president_names lists
if len(files) != len(president_names):
    print(f"Warning: Number of files ({len(files)}) does not match number of president names ({len(president_names)})")

# Initialize dataframe with appropriate column names
df = pd.DataFrame(columns=['Presidents', 'Sentences'])

# Iterate over all files and extract sentences
for file_index in range(len(files)):
    file_path = os.path.join(folder_path, files[file_index])
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()[2:]  # Adjust if your files have a different structure

    text = ' '.join(lines)
    sentences = sent_tokenize(text)
    cleaned_sentences = [sentence.replace('\n', '') for sentence in sentences]

    current_president = president_names[file_index]
    dftemp = pd.DataFrame({'Presidents': [current_president] * len(cleaned_sentences), 'Sentences': cleaned_sentences})
    df = pd.concat([df, dftemp], axis=0, ignore_index=True)

df.reset_index(drop=True, inplace=True)

# Save the DataFrame to a CSV file
#df.to_csv('finalSentence.csv', index=False)
```


```{python}
data = pd.read_csv("finalSentence.csv")
```

# EDA

```{python}
# Set the style of the visualization
sns.set(style="whitegrid")

# Group the data by president and count the number of sentences for each
sentence_counts = data['Presidents'].value_counts()

# Create a bar plot of the number of sentences per president
plt.figure(figsize=(10, 6))
sns.barplot(x=sentence_counts.index, y=sentence_counts.values, palette="viridis")

# Add labels and title
plt.xlabel('Presidents')
plt.ylabel('Number of Sentences')
plt.title('Number of Sentences per President')
plt.xticks(rotation=45)
plt.show()
```

```{python}
data = data[data['Presidents'] != ' Motlanthe']
data = data[data['Presidents'] != 'deKlerk']
```

```{python}
# Calculate the length in words of each sentence
data['Sentence_Length'] = data['Sentences'].apply(lambda x: len(x.split()))

# Group by president and calculate the average sentence length
average_length = data.groupby('Presidents')['Sentence_Length'].mean().sort_values(ascending=False)

palette = sns.color_palette("viridis", n_colors=len(sentence_counts))

# Create a mapping of president to color based on the order in sentence_counts
color_mapping = {president: palette[i] for i, president in enumerate(sentence_counts.index)}

# Get colors for the presidents in the order of average_length
bar_colors = [color_mapping[president] for president in average_length.index]

# Create a bar plot of the average sentence length per president with consistent colors
plt.figure(figsize=(10, 6))
sns.barplot(x=average_length.index, y=average_length.values, palette=bar_colors)

# Add labels and title
plt.xlabel('Presidents')
plt.ylabel('Average Sentence Length (in words)')
plt.title('Average Sentence Length per President')
plt.xticks(rotation=45)
plt.show()

```

# Overall top Words used

blah blah blah
# Word Cloud per president

```{python}
# Define a function to remove stop words from a sentence
def remove_stopwords(sentence):
    words = sentence.split()
    cleaned_words = [word for word in words if word.lower() not in STOPWORDS]
    return ' '.join(cleaned_words)

# Apply the function to remove stop words from each sentence in the DataFrame
data['Cleaned_Sentences'] = data['Sentences'].apply(remove_stopwords)

# Group the cleaned sentences by president
grouped = data.groupby('Presidents')['Cleaned_Sentences'].apply(' '.join).reset_index()

# Initialize a dictionary to store word clouds for each president
wordclouds = {}

# Generate a word cloud for each president
for index, row in grouped.iterrows():
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(row['Cleaned_Sentences'])
    wordclouds[row['Presidents']] = wordcloud

# Display the word clouds
plt.figure(figsize=(15, 10))
for i, (president, wordcloud) in enumerate(wordclouds.items(), 1):
    plt.subplot(2, 2, i)  # Adjusted subplot arrangement to 2x2 grid
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(president)
    plt.axis("off")
plt.show()
```


# Bag of Words + Data Split

ayo

```{python}
# Preprocess text data: lowercasing and removing punctuation
data['Processed_Sentences'] = data['Sentences'].str.lower().str.replace('[^\w\s]', '', regex=True)

# Extract relevant columns
text_data = data['Processed_Sentences']
y = data['Presidents']

# Initialize a CountVectorizer for BOW representation with stop words removal
vectorizer = CountVectorizer(lowercase=True, token_pattern=r"(?u)\b\w+\b", stop_words='english')

# Fit and transform the text data
X = vectorizer.fit_transform(text_data)

# Encode the class labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

```


# Boosted Tree CatBoost




Here looks like 53 percent accuracy

```{python}
# Initialize CatBoostClassifier
clf = CatBoostClassifier(
    iterations=500, # Number of boosting iterations
    depth=10,       # Depth of the trees
    learning_rate=0.05, # Learning rate
    loss_function='MultiClass', # Objective function
    random_state=42, # Random seed
    verbose=10      # Output training information every 100 iterations
)

# Fit the model on the training data
clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = clf.predict(X_test)

# Decode the predicted labels back to original classes
y_pred_decoded = le.inverse_transform(y_pred.flatten())

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred, target_names=le.classes_)

# Print the accuracy and classification report
print(f'Accuracy: {accuracy * 100:.2f}%')
print('Classification Report:')
print(classification_rep)
```

# Neural Network


```{python}
# Convert the sparse matrix to dense matrix as neural network needs dense input
X_train_dense = X_train.toarray()
X_test_dense = X_test.toarray()

# Define model
model = Sequential([
    Dense(512, input_dim=X_train_dense.shape[1], activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-2)),  # Input layer
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)),  # Hidden layer
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)),  # Hidden layer
    Dense(len(le.classes_), activation='softmax')  # Output layer
])

# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

callbacks = [early_stopping, model_checkpoint]
# Train model
model.fit(X_train_dense, y_train,  validation_split=0.2, epochs=100, callbacks=callbacks)
# Evaluate model
loss, accuracy = model.evaluate(X_test_dense, y_test)
print(f'Loss: {loss:.4f}')
print(f'Accuracy: {accuracy*100:.2f}%')
```

## SVM


```{python, eval = FALSE}
param_grid = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf'],  # Type of SVM
    'gamma': ['scale', 'auto']  # Kernel coefficient
}

# Initialize the SVM classifier
svm = SVC(random_state=42)

# Initialize Grid Search
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy')

# Perform Grid Search on the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and the best estimator from Grid Search
best_params = grid_search.best_params_
best_svm_clf = grid_search.best_estimator_

# Make predictions using the best model
y_pred_best_svm = best_svm_clf.predict(X_test)

# Evaluate the best model
accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)
classification_rep_best_svm = classification_report(y_test, y_pred_best_svm)

best_params, accuracy_best_svm, classification_rep_best_svm
```



```{python}

# Initialize the Support Vector Machine classifier
svm_clf = SVC(kernel='linear', gamma = 'scale', C = 0.1, random_state=42)

# Fit the model on the training data
svm_clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred_svm = svm_clf.predict(X_test)

# Evaluate the classifier
accuracy_svm = accuracy_score(y_test, y_pred_svm)
classification_rep_svm = classification_report(y_test, y_pred_svm)

# Print the accuracy and classification report
print(f'test score {accuracy_svm}')
```

# TF-IDF

```{python}
# Initialize a TfidfVectorizer with stop words removal
vectorizer = TfidfVectorizer(stop_words='english')

# Compute the TF-IDF values for each word in each sentence
X = vectorizer.fit_transform(data['Sentences'])

# Encode the president names as target variable
y = data['Presidents']

# Split the data into training and testing sets with an 80-20 ratio
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Display the shapes of the resulting training and testing sets
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```


## Catboost

```{python}
# If CatBoost requires numeric labels, we need to fit a LabelEncoder
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

# Initialize CatBoostClassifier
clf = CatBoostClassifier(
    iterations=500, # Number of boosting iterations
    depth=10,       # Depth of the trees
    learning_rate=0.05, # Learning rate
    loss_function='MultiClass', # Objective function
    random_state=42, # Random seed
    verbose=10      # Output training information every 10 iterations
)

# Fit the model on the training data (converting sparse matrix to dense)
clf.fit(X_train.toarray(), y_train_encoded)

# Make predictions on the testing data
y_pred = clf.predict(X_test.toarray())

# Decode the predicted labels back to original classes (if necessary)
y_pred_decoded = le.inverse_transform(y_pred.flatten().astype(int))

# Evaluate the classifier
accuracy = accuracy_score(y_test_encoded, y_pred.flatten().astype(int))
classification_rep = classification_report(y_test_encoded, y_pred.flatten().astype(int), target_names=le.classes_)

# Print the accuracy and classification report
print(f'Accuracy: {accuracy * 100:.2f}%')
print('Classification Report:')
print(classification_rep)
```

## SVM


grid search SVM

best params:
10, 'gamma': 'scale', 'kernel': 'rbf'


```{python, eval = FALSE}
param_grid = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf'],  # Type of SVM
    'gamma': ['scale', 'auto']  # Kernel coefficient
}

# Initialize the SVM classifier
svm = SVC(random_state=42)

# Initialize Grid Search
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy')

# Perform Grid Search on the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and the best estimator from Grid Search
best_params = grid_search.best_params_
best_svm_clf = grid_search.best_estimator_

# Make predictions using the best model
y_pred_best_svm = best_svm_clf.predict(X_test)

# Evaluate the best model
accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)
classification_rep_best_svm = classification_report(y_test, y_pred_best_svm)

best_params, accuracy_best_svm, classification_rep_best_svm
```

```{python}
# Initialize the Support Vector Machine classifier
svm_clf = SVC(kernel='rbf', gamma = 'scale', C = 10, random_state=42)

# Fit the model on the training data
svm_clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred_svm = svm_clf.predict(X_test)

# Evaluate the classifier
accuracy_svm = accuracy_score(y_test, y_pred_svm)
classification_rep_svm = classification_report(y_test, y_pred_svm)

# Print the accuracy and classification report
accuracy_svm
```

# Word Embeddings 


```{python}
# Tokenize sentences using white-space-based tokenization
data['Tokenized_Sentences'] = data['Sentences'].apply(lambda x: x.split())

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=data['Tokenized_Sentences'], vector_size=100, window=5, min_count=1, workers=4)

# Function to calculate sentence embedding
def sentence_embedding(sentence_tokens, model):
    """Calculate sentence embedding as the mean of the word embeddings."""
    embeddings = [model.wv[word] for word in sentence_tokens if word in model.wv.index_to_key]
    return np.mean(embeddings, axis=0) if len(embeddings) > 0 else np.zeros(model.vector_size)

# Create sentence embeddings
data['Sentence_Embeddings'] = data['Tokenized_Sentences'].apply(lambda x: sentence_embedding(x, word2vec_model))

# Encode the "Presidents" labels into numerical format
label_mapping = {label: idx for idx, label in enumerate(data['Presidents'].unique())}
data['Label'] = data['Presidents'].map(label_mapping)

# Split data into features (X) and target (y)
X = np.vstack(data['Sentence_Embeddings'].to_numpy())
y = data['Label'].to_numpy()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

```


Accuracy: 39.53%
```{python}
# Initialize CatBoostClassifier
clf = CatBoostClassifier(
    iterations=500,          # Number of boosting iterations
    depth=10,                # Depth of the trees
    learning_rate=0.05,      # Learning rate
    loss_function='MultiClass',  # Objective function
    random_state=42,             # Random seed
    verbose=10                   # Output training information every 10 iterations
)

# Fit the model on the training data
clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = clf.predict(X_test)

# Note: If y_pred is not a flat array, you might need to flatten it or adjust it to match the shape of y_test
# For example, you might need to use y_pred.flatten() or y_pred[:, 0] depending on the output of the predict method

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred.flatten().astype(int))
classification_rep = classification_report(y_test, y_pred.flatten().astype(int), target_names=label_mapping.keys())

# Print the accuracy and classification report
print(f'Accuracy: {accuracy * 100:.2f}%')
print('Classification Report:')
print(classification_rep)
```

## SVM

```{python}

param_grid = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf'],  # Type of SVM
    'gamma': ['scale', 'auto']  # Kernel coefficient
}

# Initialize the SVM classifier
svm = SVC(random_state=42)

# Initialize Grid Search
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy')

# Perform Grid Search on the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and the best estimator from Grid Search
best_params = grid_search.best_params_
best_svm_clf = grid_search.best_estimator_

# Make predictions using the best model
y_pred_best_svm = best_svm_clf.predict(X_test)

# Evaluate the best model
accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)
classification_rep_best_svm = classification_report(y_test, y_pred_best_svm, target_names=label_mapping.keys())
```


```{python}
# Initialize the Support Vector Machine classifier with specific parameters
specific_svm_clf = SVC(kernel='linear', gamma='scale', C=10, random_state=42)

# Fit the model on the training data
specific_svm_clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred_specific_svm = specific_svm_clf.predict(X_test)

# Evaluate the classifier
accuracy_specific_svm = accuracy_score(y_test, y_pred_specific_svm)
classification_rep_specific_svm = classification_report(y_test, y_pred_specific_svm, target_names=label_mapping.keys())

# Print the accuracy and classification report
print(f'Accuracy (Specific SVM): {accuracy_specific_svm * 100:.2f}%')
print('Classification Report (Specific SVM):')
print(classification_rep_specific_svm)
```


## Neural Network 
Accuracy: 36.69%

```{python}
# Define model
model = Sequential([
    Dense(512, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-2)),  # Input layer
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)),  # Hidden layer
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)),  # Hidden layer
    Dense(len(label_mapping), activation='softmax')  # Output layer
])

# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Set early stopping and model checkpoint callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)
callbacks = [early_stopping, model_checkpoint]

# Train model
model.fit(X_train, y_train, validation_split=0.2, epochs=100, callbacks=callbacks)

# Evaluate model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Loss: {loss:.4f}')
print(f'Accuracy: {accuracy*100:.2f}%')
```

