---
title: "SONA_NLP_Python"
---

# Introduction

In this paper we examine the state of the natioin address(SONA) speeches from all South African presidents from years 1994 to 2023. The aim of this paper is to classify speakers/presidents based on a single sentence extracted from a given SONA speach. The following will proceeed in XXX main sections. Firstly a small literautre review of natural language processing(NLP) will be exmained with a specific focus on NLP classification. Thereafter the data will be explored and cleaning and preprocessing steps will be detailed. Here exploratioin around the data will center on the balance of the dataset as well as the overall and president speicific vocabulary used. The third section of the paper will detail the methods used within this paper. This will include detailing the three feature extraction tools utilised in bag of words(BoW), 


```{python}
pip install nltk
```

```{python}
pip install requests matplotlib seaborn sklearn scikit-learn nltk
```

```{python}
import os
import pandas as pd
import nltk
import re
from nltk.tokenize import sent_tokenize

nltk.download('punkt')

folder_path = 'speeches'  # Ensure this is your correct folder path
files = os.listdir(folder_path)
files = sorted([file for file in files if os.path.isfile(os.path.join(folder_path, file)) and file.endswith('.txt')])

president_names = []

# Updated regex pattern to handle more cases
pattern = r'_(.+?)\.txt'  # Non-greedy match to get the president name

for file in files:
    match = re.search(pattern, file)
    if match:
        president_name = match.group(1)
        # Remove the "_2" suffix from the president names here
        cleaned_president_name = president_name.replace('_2', '')
        president_names.append(cleaned_president_name)
    else:
        print(f"Warning: No match found in filename: {file}")
        president_names.append('Unknown')  # Placeholder for missing names

# Check the lengths of files and president_names lists
if len(files) != len(president_names):
    print(f"Warning: Number of files ({len(files)}) does not match number of president names ({len(president_names)})")

# Initialize dataframe with appropriate column names
df = pd.DataFrame(columns=['Presidents', 'Sentences'])

# Iterate over all files and extract sentences
for file_index in range(len(files)):
    file_path = os.path.join(folder_path, files[file_index])
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()[2:]  # Adjust if your files have a different structure

    text = ' '.join(lines)
    sentences = sent_tokenize(text)
    cleaned_sentences = [sentence.replace('\n', '') for sentence in sentences]

    current_president = president_names[file_index]
    dftemp = pd.DataFrame({'Presidents': [current_president] * len(cleaned_sentences), 'Sentences': cleaned_sentences})
    df = pd.concat([df, dftemp], axis=0, ignore_index=True)

df.reset_index(drop=True, inplace=True)

# Save the DataFrame to a CSV file
#df.to_csv('finalSentence.csv', index=False)
```


```{python}
data = pd.read_csv("finalSentence.csv")
```

# EDA

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# Set the style of the visualization
sns.set(style="whitegrid")

# Group the data by president and count the number of sentences for each
sentence_counts = data['Presidents'].value_counts()

# Create a bar plot of the number of sentences per president
plt.figure(figsize=(10, 6))
sns.barplot(x=sentence_counts.index, y=sentence_counts.values, palette="viridis")

# Add labels and title
plt.xlabel('Presidents')
plt.ylabel('Number of Sentences')
plt.title('Number of Sentences per President')
plt.xticks(rotation=45)
plt.show()
```

```{python}
data = data[data['Presidents'] != ' Motlanthe']
data = data[data['Presidents'] != 'deKlerk']
```

```{python}
# Calculate the length in words of each sentence
data['Sentence_Length'] = data['Sentences'].apply(lambda x: len(x.split()))

# Group by president and calculate the average sentence length
average_length = data.groupby('Presidents')['Sentence_Length'].mean().sort_values(ascending=False)

palette = sns.color_palette("viridis", n_colors=len(sentence_counts))

# Create a mapping of president to color based on the order in sentence_counts
color_mapping = {president: palette[i] for i, president in enumerate(sentence_counts.index)}

# Get colors for the presidents in the order of average_length
bar_colors = [color_mapping[president] for president in average_length.index]

# Create a bar plot of the average sentence length per president with consistent colors
plt.figure(figsize=(10, 6))
sns.barplot(x=average_length.index, y=average_length.values, palette=bar_colors)

# Add labels and title
plt.xlabel('Presidents')
plt.ylabel('Average Sentence Length (in words)')
plt.title('Average Sentence Length per President')
plt.xticks(rotation=45)
plt.show()

```

# Overall top Words used

```{python}
pip install wordcloud
```

# Word Cloud per president

```{python}
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# Define a function to remove stop words from a sentence
def remove_stopwords(sentence):
    words = sentence.split()
    cleaned_words = [word for word in words if word.lower() not in STOPWORDS]
    return ' '.join(cleaned_words)

# Apply the function to remove stop words from each sentence in the DataFrame
data['Cleaned_Sentences'] = data['Sentences'].apply(remove_stopwords)

# Group the cleaned sentences by president
grouped = data.groupby('Presidents')['Cleaned_Sentences'].apply(' '.join).reset_index()

# Initialize a dictionary to store word clouds for each president
wordclouds = {}

# Generate a word cloud for each president
for index, row in grouped.iterrows():
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(row['Cleaned_Sentences'])
    wordclouds[row['Presidents']] = wordcloud

# Display the word clouds
plt.figure(figsize=(15, 10))
for i, (president, wordcloud) in enumerate(wordclouds.items(), 1):
    plt.subplot(2, 2, i)  # Adjusted subplot arrangement to 2x2 grid
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(president)
    plt.axis("off")
plt.show()
```


# Bag of Words + Data Split

```{python}
pip install scikit-learn
```


```{python}
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split


# Preprocess text data: lowercasing and removing punctuation
data['Processed_Sentences'] = data['Sentences'].str.lower().str.replace('[^\w\s]', '', regex=True)

# Extract relevant columns
text_data = data['Processed_Sentences']
y = data['Presidents']

# Initialize a CountVectorizer for BOW representation with stop words removal
vectorizer = CountVectorizer(lowercase=True, token_pattern=r"(?u)\b\w+\b", stop_words='english')

# Fit and transform the text data
X = vectorizer.fit_transform(text_data)

# Encode the class labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

```


# Boosted Tree CatBoost

```{python}
pip install catboost
```


Here looks like 53 percent accuracy

```{python}
from catboost import CatBoostClassifier
from sklearn.metrics import classification_report, accuracy_score

# Initialize CatBoostClassifier
clf = CatBoostClassifier(
    iterations=500, # Number of boosting iterations
    depth=10,       # Depth of the trees
    learning_rate=0.05, # Learning rate
    loss_function='MultiClass', # Objective function
    random_state=42, # Random seed
    verbose=10      # Output training information every 100 iterations
)

# Fit the model on the training data
clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = clf.predict(X_test)

# Decode the predicted labels back to original classes
y_pred_decoded = le.inverse_transform(y_pred.flatten())

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred, target_names=le.classes_)

# Print the accuracy and classification report
print(f'Accuracy: {accuracy * 100:.2f}%')
print('Classification Report:')
print(classification_rep)
```

# Neural Network

```{python}
pip install tensorflow
```

```{python}
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization
from keras.regularizers import l2
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint

# Convert the sparse matrix to dense matrix as neural network needs dense input
X_train_dense = X_train.toarray()
X_test_dense = X_test.toarray()

# Define model
model = Sequential([
    Dense(512, input_dim=X_train_dense.shape[1], activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-2)),  # Input layer
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)),  # Hidden layer
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)),  # Hidden layer
    Dense(len(le.classes_), activation='softmax')  # Output layer
])

# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

callbacks = [early_stopping, model_checkpoint]
# Train model
model.fit(X_train_dense, y_train,  validation_split=0.2, epochs=100, callbacks=callbacks)
# Evaluate model
loss, accuracy = model.evaluate(X_test_dense, y_test)
print(f'Loss: {loss:.4f}')
print(f'Accuracy: {accuracy*100:.2f}%')
```

## SVM


```{python}
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf'],  # Type of SVM
    'gamma': ['scale', 'auto']  # Kernel coefficient
}

# Initialize the SVM classifier
svm = SVC(random_state=42)

# Initialize Grid Search
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy')

# Perform Grid Search on the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and the best estimator from Grid Search
best_params = grid_search.best_params_
best_svm_clf = grid_search.best_estimator_

# Make predictions using the best model
y_pred_best_svm = best_svm_clf.predict(X_test)

# Evaluate the best model
accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)
classification_rep_best_svm = classification_report(y_test, y_pred_best_svm)

best_params, accuracy_best_svm, classification_rep_best_svm
```



```{python}
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Initialize the Support Vector Machine classifier
svm_clf = SVC(kernel='linear', gamma = 'scale', C = 0.1, random_state=42)

# Fit the model on the training data
svm_clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred_svm = svm_clf.predict(X_test)

# Evaluate the classifier
accuracy_svm = accuracy_score(y_test, y_pred_svm)
classification_rep_svm = classification_report(y_test, y_pred_svm)

# Print the accuracy and classification report
print(f'test score {accuracy_svm}')
```

# TF-IDF

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Initialize a TfidfVectorizer with stop words removal
vectorizer = TfidfVectorizer(stop_words='english')

# Compute the TF-IDF values for each word in each sentence
X = vectorizer.fit_transform(data['Sentences'])

# Encode the president names as target variable
y = data['Presidents']

# Split the data into training and testing sets with an 80-20 ratio
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Display the shapes of the resulting training and testing sets
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```


## Catboost

```{python}
from catboost import CatBoostClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# If CatBoost requires numeric labels, we need to fit a LabelEncoder
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

# Initialize CatBoostClassifier
clf = CatBoostClassifier(
    iterations=500, # Number of boosting iterations
    depth=10,       # Depth of the trees
    learning_rate=0.05, # Learning rate
    loss_function='MultiClass', # Objective function
    random_state=42, # Random seed
    verbose=10      # Output training information every 10 iterations
)

# Fit the model on the training data (converting sparse matrix to dense)
clf.fit(X_train.toarray(), y_train_encoded)

# Make predictions on the testing data
y_pred = clf.predict(X_test.toarray())

# Decode the predicted labels back to original classes (if necessary)
y_pred_decoded = le.inverse_transform(y_pred.flatten().astype(int))

# Evaluate the classifier
accuracy = accuracy_score(y_test_encoded, y_pred.flatten().astype(int))
classification_rep = classification_report(y_test_encoded, y_pred.flatten().astype(int), target_names=le.classes_)

# Print the accuracy and classification report
print(f'Accuracy: {accuracy * 100:.2f}%')
print('Classification Report:')
print(classification_rep)
```

## SVM


grid search SVM

best params:
10, 'gamma': 'scale', 'kernel': 'rbf'}
```{python}
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf'],  # Type of SVM
    'gamma': ['scale', 'auto']  # Kernel coefficient
}

# Initialize the SVM classifier
svm = SVC(random_state=42)

# Initialize Grid Search
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy')

# Perform Grid Search on the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and the best estimator from Grid Search
best_params = grid_search.best_params_
best_svm_clf = grid_search.best_estimator_

# Make predictions using the best model
y_pred_best_svm = best_svm_clf.predict(X_test)

# Evaluate the best model
accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)
classification_rep_best_svm = classification_report(y_test, y_pred_best_svm)

best_params, accuracy_best_svm, classification_rep_best_svm
```

```{python}
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Initialize the Support Vector Machine classifier
svm_clf = SVC(kernel='rbf', gamma = 'scale', C = 10, random_state=42)

# Fit the model on the training data
svm_clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred_svm = svm_clf.predict(X_test)

# Evaluate the classifier
accuracy_svm = accuracy_score(y_test, y_pred_svm)
classification_rep_svm = classification_report(y_test, y_pred_svm)

# Print the accuracy and classification report
accuracy_svm
```

# Word Embeddings 

```{python}
pip install gensim
```

```{python}
import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split


# Tokenize sentences using white-space-based tokenization
data['Tokenized_Sentences'] = data['Sentences'].apply(lambda x: x.split())

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=data['Tokenized_Sentences'], vector_size=100, window=5, min_count=1, workers=4)

# Function to calculate sentence embedding
def sentence_embedding(sentence_tokens, model):
    """Calculate sentence embedding as the mean of the word embeddings."""
    embeddings = [model.wv[word] for word in sentence_tokens if word in model.wv.index_to_key]
    return np.mean(embeddings, axis=0) if len(embeddings) > 0 else np.zeros(model.vector_size)

# Create sentence embeddings
data['Sentence_Embeddings'] = data['Tokenized_Sentences'].apply(lambda x: sentence_embedding(x, word2vec_model))

# Encode the "Presidents" labels into numerical format
label_mapping = {label: idx for idx, label in enumerate(data['Presidents'].unique())}
data['Label'] = data['Presidents'].map(label_mapping)

# Split data into features (X) and target (y)
X = np.vstack(data['Sentence_Embeddings'].to_numpy())
y = data['Label'].to_numpy()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

```


Accuracy: 39.53%
```{python}
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, classification_report
# Initialize CatBoostClassifier
clf = CatBoostClassifier(
    iterations=500,          # Number of boosting iterations
    depth=10,                # Depth of the trees
    learning_rate=0.05,      # Learning rate
    loss_function='MultiClass',  # Objective function
    random_state=42,             # Random seed
    verbose=10                   # Output training information every 10 iterations
)

# Fit the model on the training data
clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = clf.predict(X_test)

# Note: If y_pred is not a flat array, you might need to flatten it or adjust it to match the shape of y_test
# For example, you might need to use y_pred.flatten() or y_pred[:, 0] depending on the output of the predict method

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred.flatten().astype(int))
classification_rep = classification_report(y_test, y_pred.flatten().astype(int), target_names=label_mapping.keys())

# Print the accuracy and classification report
print(f'Accuracy: {accuracy * 100:.2f}%')
print('Classification Report:')
print(classification_rep)
```

## SVM

```{python}
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf'],  # Type of SVM
    'gamma': ['scale', 'auto']  # Kernel coefficient
}

# Initialize the SVM classifier
svm = SVC(random_state=42)

# Initialize Grid Search
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy')

# Perform Grid Search on the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and the best estimator from Grid Search
best_params = grid_search.best_params_
best_svm_clf = grid_search.best_estimator_

# Make predictions using the best model
y_pred_best_svm = best_svm_clf.predict(X_test)

# Evaluate the best model
accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)
classification_rep_best_svm = classification_report(y_test, y_pred_best_svm, target_names=label_mapping.keys())
```


```{python}
# Initialize the Support Vector Machine classifier with specific parameters
specific_svm_clf = SVC(kernel='linear', gamma='scale', C=10, random_state=42)

# Fit the model on the training data
specific_svm_clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred_specific_svm = specific_svm_clf.predict(X_test)

# Evaluate the classifier
accuracy_specific_svm = accuracy_score(y_test, y_pred_specific_svm)
classification_rep_specific_svm = classification_report(y_test, y_pred_specific_svm, target_names=label_mapping.keys())

# Print the accuracy and classification report
print(f'Accuracy (Specific SVM): {accuracy_specific_svm * 100:.2f}%')
print('Classification Report (Specific SVM):')
print(classification_rep_specific_svm)
```


## Neural Network 
Accuracy: 36.69%

```{python}
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Define model
model = Sequential([
    Dense(512, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-2)),  # Input layer
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)),  # Hidden layer
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)),  # Hidden layer
    Dense(len(label_mapping), activation='softmax')  # Output layer
])

# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Set early stopping and model checkpoint callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)
callbacks = [early_stopping, model_checkpoint]

# Train model
model.fit(X_train, y_train, validation_split=0.2, epochs=100, callbacks=callbacks)

# Evaluate model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Loss: {loss:.4f}')
print(f'Accuracy: {accuracy*100:.2f}%')
```

