---
title: "SONA_NLP_Python"
---
# Introduction

```{python}
pip install nltk
```
```{python}
pip install requests matplotlib seaborn sklearn scikit-learn nltk
```

```{python}
import os
import pandas as pd
import nltk
import re
from nltk.tokenize import sent_tokenize

nltk.download('punkt')

folder_path = 'speeches'  # Ensure this is your correct folder path
files = os.listdir(folder_path)
files = sorted([file for file in files if os.path.isfile(os.path.join(folder_path, file)) and file.endswith('.txt')])

president_names = []

# Updated regex pattern to handle more cases
pattern = r'_(.+?)\.txt'  # Non-greedy match to get the president name

for file in files:
    match = re.search(pattern, file)
    if match:
        president_name = match.group(1)
        # Remove the "_2" suffix from the president names here
        cleaned_president_name = president_name.replace('_2', '')
        president_names.append(cleaned_president_name)
    else:
        print(f"Warning: No match found in filename: {file}")
        president_names.append('Unknown')  # Placeholder for missing names

# Check the lengths of files and president_names lists
if len(files) != len(president_names):
    print(f"Warning: Number of files ({len(files)}) does not match number of president names ({len(president_names)})")

# Initialize dataframe with appropriate column names
df = pd.DataFrame(columns=['Presidents', 'Sentences'])

# Iterate over all files and extract sentences
for file_index in range(len(files)):
    file_path = os.path.join(folder_path, files[file_index])
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()[2:]  # Adjust if your files have a different structure

    text = ' '.join(lines)
    sentences = sent_tokenize(text)
    cleaned_sentences = [sentence.replace('\n', '') for sentence in sentences]

    current_president = president_names[file_index]
    dftemp = pd.DataFrame({'Presidents': [current_president] * len(cleaned_sentences), 'Sentences': cleaned_sentences})
    df = pd.concat([df, dftemp], axis=0, ignore_index=True)

df.reset_index(drop=True, inplace=True)

# Save the DataFrame to a CSV file
#df.to_csv('finalSentence.csv', index=False)
```


```{python}
data = pd.read_csv("finalSentence.csv")
```

# EDA

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# Set the style of the visualization
sns.set(style="whitegrid")

# Group the data by president and count the number of sentences for each
sentence_counts = data['Presidents'].value_counts()

# Create a bar plot of the number of sentences per president
plt.figure(figsize=(10, 6))
sns.barplot(x=sentence_counts.index, y=sentence_counts.values, palette="viridis")

# Add labels and title
plt.xlabel('Presidents')
plt.ylabel('Number of Sentences')
plt.title('Number of Sentences per President')
plt.xticks(rotation=45)
plt.show()
```

```{python}
data = data[data['Presidents'] != ' Motlanthe']
data = data[data['Presidents'] != 'deKlerk']
```

```{python}
# Calculate the length in words of each sentence
data['Sentence_Length'] = data['Sentences'].apply(lambda x: len(x.split()))

# Group by president and calculate the average sentence length
average_length = data.groupby('Presidents')['Sentence_Length'].mean().sort_values(ascending=False)

palette = sns.color_palette("viridis", n_colors=len(sentence_counts))

# Create a mapping of president to color based on the order in sentence_counts
color_mapping = {president: palette[i] for i, president in enumerate(sentence_counts.index)}

# Get colors for the presidents in the order of average_length
bar_colors = [color_mapping[president] for president in average_length.index]

# Create a bar plot of the average sentence length per president with consistent colors
plt.figure(figsize=(10, 6))
sns.barplot(x=average_length.index, y=average_length.values, palette=bar_colors)

# Add labels and title
plt.xlabel('Presidents')
plt.ylabel('Average Sentence Length (in words)')
plt.title('Average Sentence Length per President')
plt.xticks(rotation=45)
plt.show()

```

# Overall top Words used
```{python}
from collections import Counter

# Combine all cleaned sentences into a single string
all_cleaned_sentences = ' '.join(df['Cleaned_Sentences'])

# Tokenize the string into individual words
all_words = all_cleaned_sentences.split()

# Count the frequency of each word
word_counter = Counter(all_words)

# Select the top 20 most frequent non-stopwords
top_words = word_counter.most_common(20)

# Extract words and their corresponding frequencies for plotting
words, frequencies = zip(*top_words)

# Create a bar plot representing the frequency of each of the selected words
plt.figure(figsize=(15, 7))
plt.bar(words, frequencies, color='skyblue')
plt.title('Top 20 Most Common Non-Stopwords Across All Presidents')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()
```

```{python}
pip install wordcloud
```

# Word Cloud per president

```{python}
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# Define a function to remove stop words from a sentence
def remove_stopwords(sentence):
    words = sentence.split()
    cleaned_words = [word for word in words if word.lower() not in STOPWORDS]
    return ' '.join(cleaned_words)

# Apply the function to remove stop words from each sentence in the DataFrame
data['Cleaned_Sentences'] = data['Sentences'].apply(remove_stopwords)

# Group the cleaned sentences by president
grouped = data.groupby('Presidents')['Cleaned_Sentences'].apply(' '.join).reset_index()

# Initialize a dictionary to store word clouds for each president
wordclouds = {}

# Generate a word cloud for each president
for index, row in grouped.iterrows():
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(row['Cleaned_Sentences'])
    wordclouds[row['Presidents']] = wordcloud

# Display the word clouds
plt.figure(figsize=(15, 10))
for i, (president, wordcloud) in enumerate(wordclouds.items(), 1):
    plt.subplot(2, 2, i)  # Adjusted subplot arrangement to 2x2 grid
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(president)
    plt.axis("off")
plt.show()
```


# Bag of Words + Data Split

```{python}
pip install scikit-learn
```


```{python}
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split


# Preprocess text data: lowercasing and removing punctuation
data['Processed_Sentences'] = data['Sentences'].str.lower().str.replace('[^\w\s]', '', regex=True)

# Extract relevant columns
text_data = data['Processed_Sentences']
y = data['Presidents']

# Initialize a CountVectorizer for BOW representation with stop words removal
vectorizer = CountVectorizer(lowercase=True, token_pattern=r"(?u)\b\w+\b", stop_words='english')

# Fit and transform the text data
X = vectorizer.fit_transform(text_data)

# Encode the class labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

```


# Boosted Tree CatBoost

```{python}
pip install catboost
```


Here looks like 53 percent accuracy

```{python}
from catboost import CatBoostClassifier
from sklearn.metrics import classification_report, accuracy_score

# Initialize CatBoostClassifier
clf = CatBoostClassifier(
    iterations=500, # Number of boosting iterations
    depth=10,       # Depth of the trees
    learning_rate=0.05, # Learning rate
    loss_function='MultiClass', # Objective function
    random_state=42, # Random seed
    verbose=10      # Output training information every 100 iterations
)

# Fit the model on the training data
clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = clf.predict(X_test)

# Decode the predicted labels back to original classes
y_pred_decoded = le.inverse_transform(y_pred.flatten())

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred, target_names=le.classes_)

# Print the accuracy and classification report
print(f'Accuracy: {accuracy * 100:.2f}%')
print('Classification Report:')
print(classification_rep)
```

# Neural Network

```{python}
pip install tensorflow
```

```{python}
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization
from keras.regularizers import l2
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint

# Convert the sparse matrix to dense matrix as neural network needs dense input
X_train_dense = X_train.toarray()
X_test_dense = X_test.toarray()

# Define model
model = Sequential([
    Dense(512, input_dim=X_train_dense.shape[1], activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)),  # Input layer
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)),  # Hidden layer
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-2)),  # Hidden layer
    Dense(len(le.classes_), activation='softmax')  # Output layer
])

# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

callbacks = [early_stopping, model_checkpoint]
# Train model
model.fit(X_train_dense, y_train,  validation_split=0.2, epochs=100, callbacks=callbacks)
# Evaluate model
loss, accuracy = model.evaluate(X_test_dense, y_test)
print(f'Loss: {loss:.4f}')
print(f'Accuracy: {accuracy*100:.2f}%')
```

