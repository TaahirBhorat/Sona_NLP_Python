[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SONA_NLP_Python",
    "section": "",
    "text": "This paper critically analyzes the State of the Nation Address (SONA) speeches delivered by various South African presidents from 1994 to 2023. The primary objective is to categorize each president based on single sentences extracted from their respective SONA speeches. The study unfolds in XXX main sections.\nInitially, a concise literature review is presented, with emphasis on the domain of Natural Language Processing (NLP), particularly focusing on classification tasks within NLP. This review lays the groundwork for the methodologies and approaches applied in later sections of the paper.\nSubsequent sections offer an in-depth exploration and meticulous cleaning of the data utilized in the study. The exploration phase scrutinizes the dataset’s balance and analyzes the vocabulary used, both overall and by each specific president. These preliminary steps are crucial for ensuring the integrity and reliability of the study’s findings.\nThe paper then transitions to a detailed exposition of the methodologies employed in the study. The methods section elucidates the three feature extraction tools deployed: Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Word Tokenization. Additionally, it describes the XXX predictive models applied, namely Gradient Boosted Trees, Feed Forward Neural Networks, and Support Vector Machines. Each tool and model is presented with a rationale for its inclusion and an explanation of its contribution to the study’s objectives.\nFollowing the methods section, the paper presents and succinctly discusses the study’s results. This section provides an initial interpretation of the findings, preparing the ground for the more in-depth analysis that follows.\nIn the penultimate section, a comprehensive discussion of the results is provided. This discussion delves into the insights gleaned from the findings, offering detailed interpretations and drawing connections with the literature reviewed earlier. This section aims not only to shed light on the study’s findings but also to locate these within the broader academic discourse on the subject.\nFinally, the paper concludes with a reflective overview of the study as a whole. This concluding section evaluates the study’s successes and limitations, reflects on its contributions to the field, and suggests avenues for future research and exploration. Through this reflective lens, the paper not only summarizes its findings but also invites further scholarly engagement with the questions and challenges raised during the study."
  },
  {
    "objectID": "index.html#data-pre-processing",
    "href": "index.html#data-pre-processing",
    "title": "SONA_NLP_Python",
    "section": "Data Pre-Processing",
    "text": "Data Pre-Processing\nIn the data preprocessing phase prior to feature extraction, initial data loading was accomplished from text files, with each file containing SONA speeches from different South African presidents from 1994 to 2023. Files were filtered to ensure they were valid, and the president’s names were extracted and cleaned for later use. The speeches within each file were tokenized into sentences using the NLTK library, and any unnecessary newline characters within these sentences were removed. Each sentence was then associated with the relevant president, resulting in a structured data frame containing each sentence alongside its corresponding president’s name. Following this, the data underwent exploratory data analysis (EDA) where sentences associated with specific presidents were filtered out, and sentence lengths were calculated and visualized. The sentences were further cleaned by removing stop words (common words that do not contribute to the meaning of a sentence), and then grouped by president. Subsequently, the cleaned sentences were used for generating word clouds for visual inspection. Furthermore the most common words across all presidents were calculated and the average words per sentence was computed and plotted for each preesident."
  },
  {
    "objectID": "index.html#feature-extraction",
    "href": "index.html#feature-extraction",
    "title": "SONA_NLP_Python",
    "section": "Feature Extraction",
    "text": "Feature Extraction\n\nBag of Words (BoW)\nThe Bag of Words (BoW) method represents text data as a matrix of token (typically words) occurrence within a given document. Each row of the matrix corresponds to a document, while each column represents a unique token in the dataset. The matrix cell contains the count of occurrences of the token in the document. In mathematical terms, for a set of \\(n\\) documents \\(D\\) and a set of \\(m\\) unique tokens \\(T\\), the BoW matrix \\(M\\) is a \\(n \\times m\\) matrix where \\(M_{ij}\\) is the frequency of token \\(j\\) in document \\(i\\). For the dataset in question, each sentence from the president’s speeches is treated as a document. The BoW model tokenizes each sentence into words, creating a matrix that reflects the frequency count of each word within each sentence, yielding a sparse matrix representation of the word distribution in each president’s speech.\n\n\nTerm Frequency-Inverse Document Frequency (TF-IDF)\nThe Term Frequency-Inverse Document Frequency (TF-IDF) technique assigns a weight to each term in a document reflecting its importance in the document relative to the entire corpus. The TF-IDF value of a term \\(t\\) in a document \\(d\\) within a corpus \\(D\\) is computed as \\(\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\\), where \\(\\text{TF}(t, d)\\) is the frequency of term \\(t\\) in document \\(d\\) divided by the total number of terms in \\(d\\), and \\(\\text{IDF}(t, D)\\) is the logarithm of the total number of documents in \\(D\\) divided by the number of documents containing term \\(t\\). In the context of the dataset, TF-IDF is calculated for each term in every sentence, resulting in a vector of TF-IDF values for each sentence, thereby emphasizing terms that are distinctive to specific speeches or presidents.\n\n\nWord Embeddings\nSentences from the presidential speeches dataset are tokenized into words. These tokens are then fed into the Word2Vec model, which learns vector representations for each word by predicting the context in which a word appears, effectively capturing the semantic relationships between words.Upon training Word2Vec with the tokenized sentences, each word is represented as a high-dimensional vector. To form a representative vector for a complete sentence, the word vectors within each sentence are averaged. This results in a single vector per sentence, encapsulating the semantic essence of the sentence based on its constituent words.\nThese sentence-level vectors serve as the dataset’s numerical features, providing a semantically rich representation of the sentences for subsequent machine learning applications in the project. Each vector not only represents its sentence but also mirrors the inherent semantic structure and relationships within the text, offering a meaningful feature set for analysis.By employing these methods, the raw textual data from the speeches is transformed into a numerical format suitable for training machine learning models, with each technique capturing different aspects and nuances of the data’s structure and semantics."
  },
  {
    "objectID": "index.html#modelling",
    "href": "index.html#modelling",
    "title": "SONA_NLP_Python",
    "section": "Modelling",
    "text": "Modelling\nNote that for each of the following predictive models, each of the three feature extraction methods detailed above are applied.\n\nGradient Boosted Trees\nGradient Boosting is a general technique where models are built sequentially, with each new model being trained to correct the mistakes of the combined ensemble of existing models. This process is iteratively repeated, progressively improving the model’s accuracy until further improvements are negligible.\nGradient Boosted Trees (GBTs) inherently leverage an ensemble methodology, combining the predictive power of multiple weak learners, in decision trees, to create a more accurate and robust model. The algorithm incrementally builds an ensemble of trees where each subsequent tree compensates for the errors of the aggregate set of preceding trees. Through this iterative refinement, the algorithm not only enhances its precision but also avoids overfitting, providing a reliable generalization to unseen data.\nIn our deployment of GBTs, using Yandex’s CatBoost, careful parameter tuning was essential for optimized performance. We engaged 500 boosting iterations, a 0.05 learning rate, and a tree depth of 10. The boosting iterations define the number of trees in the model, with each iteration adding a new tree that corrects the errors of the ensemble. The learning rate, alternatively known as shrinkage, moderates the influence of each tree, preventing any single tree from dominating the ensemble prediction. The tree depth, meanwhile, influences the model’s complexity, with deeper trees allowing for the capture of more complex patterns in the data but at the risk of overfitting. These parameters were selected after running a grid search over hyperparameters choosing the hyperamters which minimised validation error computed using 5-fold cross validation.\n\n\nSupport Vector Machines\nSupport Vector Machines (SVMs) is supervised learning algorithm used for classification tasks. SVMs operate by constructing hyperplanes in a multidimensional space that separates cases of different class labels(). The SVM algorithm is implemented in practice using a kernel. The kernel trick helps in fitting the maximum-margin hyperplane in the transformed feature space. The hyperplane is selected to segregate classes in the best possible way.\nIn this project, SVM classifiers were employed with various parameter configurations, and their performance was evaluated based on the processed data obtained through the feature extractoin methods previously mentoined. A grid search approach was utilized to systematically explore a range of parameter values and identify the optimal configuration for each data representation method. The grid search was performed over a range of values for the C parameter, different kernel types, and the gamma coefficient for the Radial Basis Function (RBF) kernel. Specifically, the C parameter was explored over the values [0.1, 1, 10]. The C parameter represents the regularization term, controlnig the trade-off between having a smooth decision boundary and classifying the training points correctly. A smaller value of C creates a wider margin but may misclassify more points, while a larger C aims for correct classification, potentially leading to a more complex model.\nThe kernel parameter, with values ['linear', 'rbf'], specifies the type of hyperplane used to separate the data. The linear kernel is often used when the data is linearly separable, meaning it can be separated by a single line. The RBF kernel is used for non-linear data, transforming it into a higher-dimensional space where it becomes linearly separable. The gamma parameter, with values ['scale', 'auto'], defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’().\nFor the SVM model trained on data represented using Bag of Words, the grid search identified the optimal parameters to be C=0.1, with a linear kernel and gamma='scale'. When the data was represented using TF-IDF, the optimal SVM model parameters were found to be C=10, using an RBF kernel with gamma='scale'. Finally, for the Word Embeddings representation, the best-performing SVM model utilized a C value of 10, a linear kernel, and gamma='scale'.\n\n\nNeural Networks\nThe Neural Network (NN) model developed for the classification task is a multilayer perceptron (MLP), which is a type of feedforward neural network. The input data was converted from a sparse to a dense matrix format, as the latter is requisite for the functioning of neural networks. The architecture employed was the same across all feature extractioin methods as varrynig the model architecture did not prove to yield any siignificant differnce in model perrformance across the feature extraction methods.\nThe architecture of the NN consists of an input layer, a hidden layer, and an output layer. The input layer has 505 neurons, with the Rectified Linear Unit (ReLU) serving as the activation function. ReLU was chosen due to its effectiveness in mitigating the vanishing gradient problem, which is prevalent in deep networks. For the initialization of weights in the input layer, the He normal initializer was used. This initializer assists in breaking symmetry and facilitating the learning process. Additionally, to avoid the overfitting phenomenon, an L2 regularization term with a coefficient of \\(1 \\times 10^{-8}\\) was applied.\nThe hidden layer, similar to the input layer, uses the ReLU activation function and the He normal initializer, but with 220 neurons. An L2 regularization term with a coefficient of \\(1 \\times 10^{-7}\\) was incorporated to constrain the weights, providing a balance between fitting the training data and maintaining model generalization.\nThe output layer possesses neurons equal to the number of unique labels in the target variable. The softmax activation function is applied in this layer to output a probability distribution over the classes, making it suitable for multi-class classification tasks.\nDuring the training phase, a stratified 5-fold cross-validation strategy was employed. For each fold, the data was split into training and validation subsets, with the model being trained for 20 epochs. The sparse categorical crossentropy was selected as the loss function, being appropriate for multi-class classification problems. The Adam optimizer was utilized for its adaptive learning rate capabilities, facilitating a faster and more stable convergence to the minimum of the loss function. Following examination of the plot of training versus validation accuracy over epochs the optimal number of epochs fro training was set, and the final model was trainined and subsequently evaluated on the unseen test data."
  },
  {
    "objectID": "index.html#model-evaluation-metrics",
    "href": "index.html#model-evaluation-metrics",
    "title": "SONA_NLP_Python",
    "section": "Model Evaluation Metrics",
    "text": "Model Evaluation Metrics\nThe evaluation of the models’ performance employed a suite of metrics to ensure a comprehensive assessment. The primary metric used is accuracy, defined as the ratio of correctly predicted observations to the total observations:\n[ = ]\nWhile accuracy is a straightforward and informative metric, it may not be sufficiently descriptive in situations where the classes are imbalanced. For this reason, additional evaluative metrics were incorporated into the assessment framework.\nThe stratified k-fold cross-validation (with ( k = 5 )) technique used during the training phase of both Support Vector Machines (SVMs) and Neural Networks (NNs) facilitated the calculation of accuracy for both training and validation sets across all folds. The average accuracy computed from these folds provided a robust estimate of each model’s generalization performance.\nSubsequently, confusion matrices were generated for the models. A confusion matrix is a table used to describe the performance of a classification algorithm. Each row of the matrix represents the instances of the actual class, while each column represents the instances of the predicted class. The entries on the main diagonal of the confusion matrix correspond to correct predictions, whereas other entries reflect the mistakes made by the classifier. Through the confusion matrix, various metrics including precision, recall, and the F1-score were computed to provide a detailed understanding of the model’s performance. Precision (the ratio of true positive predictions to the sum of true positive and false positive predictions), recall (the ratio of true positive predictions to the sum of true positive and false negative predictions), and the F1-score (the harmonic mean of precision and recall) offer deeper insights into the reliability and effectiveness of the models, especially in the presence of class imbalance.\nFor the Gradient Boosted Trees model, the loss function was an essential aspect of evaluation. The loss function is a measure used to estimate the discrepancy between the predicted values and actual values. During the training process, the algorithm optimizes the model parameters to minimize this loss function, thereby improving the model’s predictive performance. Monitoring the change in the value of the loss function during the training process provides insights into the convergence and stability of the model, helping in understanding whether the model is learning effectively from the training data. For each training epoch, the loss was computed and analyzed to ensure the model was converging to a solution that minimized prediction errors.\n\npip install nltk requests matplotlib seaborn sklearn scikit-learn wordcloud catboost tensorflow gensim\n\nRequirement already satisfied: nltk in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (3.8.1)\nRequirement already satisfied: requests in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (2.28.1)\nRequirement already satisfied: matplotlib in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (3.8.0)\nRequirement already satisfied: seaborn in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (0.13.0)\n\n\nCollecting sklearn\n  Using cached sklearn-0.0.post9.tar.gz (3.6 kB)\n\n\n  Preparing metadata (setup.py) ... -\n\n\n\b \bdone\nRequirement already satisfied: scikit-learn in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (1.3.1)\nRequirement already satisfied: wordcloud in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (1.9.2)\nRequirement already satisfied: catboost in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: tensorflow in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (2.14.0)\nRequirement already satisfied: gensim in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (4.3.2)\nRequirement already satisfied: click in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: joblib in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from nltk) (1.3.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from nltk) (2023.8.8)\n\n\nRequirement already satisfied: tqdm in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from nltk) (4.66.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from requests) (1.26.11)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from requests) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from requests) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from requests) (2.1.1)\nRequirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib) (0.12.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib) (10.0.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib) (4.43.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.21 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib) (1.24.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib) (1.1.1)\nRequirement already satisfied: pandas>=1.2 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from seaborn) (2.1.1)\n\n\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: scipy>=1.5.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from scikit-learn) (1.9.0)\nRequirement already satisfied: plotly in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from catboost) (5.17.0)\nRequirement already satisfied: six in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from catboost) (1.16.0)\nRequirement already satisfied: graphviz in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from catboost) (0.20.1)\n\n\nRequirement already satisfied: tensorboard<2.15,>=2.14 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (2.14.1)\nRequirement already satisfied: setuptools in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (65.2.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: keras<2.15,>=2.14.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (2.14.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (4.24.4)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (2.14.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (3.7.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (1.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (4.3.0)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (1.59.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (0.34.0)\nRequirement already satisfied: ml-dtypes==0.2.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n\n\nRequirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from gensim) (6.4.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n\n\nRequirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2023.3)\nRequirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n\n\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.23.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.2.2)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.4.1)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n\n\nRequirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from plotly->catboost) (8.2.3)\n\n\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.2.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n\n\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.0)\n\n\nBuilding wheels for collected packages: sklearn\n\n\n  Building wheel for sklearn (setup.py) ... -\n\n\n\b \b\\\n\n\n\b \bdone\n  Created wheel for sklearn: filename=sklearn-0.0.post9-py3-none-any.whl size=2951 sha256=d089c2c54630a7fffac985d83987a6b435b4cae10e7bca31ac3daf0921a2c64f\n  Stored in directory: /Users/taahir/Library/Caches/pip/wheels/33/a3/d2/092b519e9522b4c91608b7dcec0dd9051fa1bff4c45f4502d1\nSuccessfully built sklearn\n\n\nInstalling collected packages: sklearn\n\n\nSuccessfully installed sklearn-0.0.post9\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip install wordcloud\n\nRequirement already satisfied: wordcloud in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (1.9.2)\n\n\nRequirement already satisfied: matplotlib in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from wordcloud) (3.8.0)\nRequirement already satisfied: numpy>=1.6.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from wordcloud) (1.24.4)\nRequirement already satisfied: pillow in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from wordcloud) (10.0.1)\nRequirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.12.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.43.0)\nRequirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib->wordcloud) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.0.9)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.5)\n\n\nRequirement already satisfied: six>=1.5 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip install gensim\n\nRequirement already satisfied: gensim in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (4.3.2)\n\n\nRequirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from gensim) (1.9.0)\nRequirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from gensim) (1.24.4)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from gensim) (6.4.0)\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip install tensorflow==2.14.0\n\nRequirement already satisfied: tensorflow==2.14.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (2.14.0)\nRequirement already satisfied: numpy>=1.23.5 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.24.4)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (0.34.0)\nRequirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (2.14.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.6.3)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (0.2.0)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (23.5.26)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.2.0)\n\n\nRequirement already satisfied: ml-dtypes==0.2.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (0.2.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (4.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (4.24.4)\nRequirement already satisfied: tensorboard<2.15,>=2.14 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (2.14.1)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (0.4.0)\nRequirement already satisfied: setuptools in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (65.2.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.14.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (3.7.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (3.3.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (16.0.6)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.59.0)\nRequirement already satisfied: keras<2.15,>=2.14.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (2.14.0)\nRequirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.16.0)\nRequirement already satisfied: packaging in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (21.3)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.1.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.37.1)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.1)\n\n\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.2.2)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.28.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.23.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from packaging->tensorflow==2.14.0) (3.0.9)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.2.7)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.2.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.3.1)\n\n\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2022.6.15)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.1.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.0)\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip install keras==2.14.0\n\nRequirement already satisfied: keras==2.14.0 in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (2.14.0)\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip install joblib\n\nRequirement already satisfied: joblib in /opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages (1.3.2)\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip install pickle\n\nERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\nERROR: No matching distribution found for pickle\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n# General imports\nimport pickle\nfrom joblib import dump, load\nimport os\nimport pandas as pd\nimport re\nimport numpy as np\n\n# NLTK imports\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\n\n# Visualization imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Preprocessing imports\nfrom sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Model selection imports\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# Machine learning model imports\nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom catboost import Pool, cv, CatBoostClassifier\n\n# Word embedding imports\nfrom gensim.models import Word2Vec\n\n# Metrics import\nfrom sklearn.metrics import classification_report, accuracy_score\n\n[nltk_data] Downloading package punkt to /Users/taahir/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n2023-10-07 19:25:11.946910: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nfolder_path = 'speeches'  # Ensure this is your correct folder path\nfiles = os.listdir(folder_path)\nfiles = sorted([file for file in files if os.path.isfile(os.path.join(folder_path, file)) and file.endswith('.txt')])\n\npresident_names = []\n\n# Updated regex pattern to handle more cases\npattern = r'_(.+?)\\.txt'  # Non-greedy match to get the president name\n\nfor file in files:\n    match = re.search(pattern, file)\n    if match:\n        president_name = match.group(1)\n        # Remove the \"_2\" suffix from the president names here\n        cleaned_president_name = president_name.replace('_2', '')\n        president_names.append(cleaned_president_name)\n    else:\n        print(f\"Warning: No match found in filename: {file}\")\n        president_names.append('Unknown')  # Placeholder for missing names\n\n# Check the lengths of files and president_names lists\nif len(files) != len(president_names):\n    print(f\"Warning: Number of files ({len(files)}) does not match number of president names ({len(president_names)})\")\n\n# Initialize dataframe with appropriate column names\ndf = pd.DataFrame(columns=['Presidents', 'Sentences'])\n\n# Iterate over all files and extract sentences\nfor file_index in range(len(files)):\n    file_path = os.path.join(folder_path, files[file_index])\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()[2:]  # Adjust if your files have a different structure\n\n    text = ' '.join(lines)\n    sentences = sent_tokenize(text)\n    cleaned_sentences = [sentence.replace('\\n', '') for sentence in sentences]\n\n    current_president = president_names[file_index]\n    dftemp = pd.DataFrame({'Presidents': [current_president] * len(cleaned_sentences), 'Sentences': cleaned_sentences})\n    df = pd.concat([df, dftemp], axis=0, ignore_index=True)\n\ndf.reset_index(drop=True, inplace=True)\n\n# Save the DataFrame to a CSV file\n#df.to_csv('finalSentence.csv', index=False)\n\n\ndata = pd.read_csv(\"finalSentence.csv\")"
  },
  {
    "objectID": "index.html#svm",
    "href": "index.html#svm",
    "title": "SONA_NLP_Python",
    "section": "SVM",
    "text": "SVM\n\n#param_grid = {\n#    'C': [0.1, 1, 10],  # Regularization parameter\n#    'kernel': ['linear', 'rbf'],  # Type of SVM\n#    'gamma': ['scale', 'auto']  # Kernel coefficient\n#}\n\n# Initialize the SVM classifier\n#svm = SVC(random_state=42)\n\n# Initialize Grid Search\n#grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy')\n\n# Perform Grid Search on the training data\n#grid_search.fit(X_train, y_train)\n\n# Get the best parameters and the best estimator from Grid Search\n#best_params = grid_search.best_params_\n#best_svm_clf = grid_search.best_estimator_\n\n# Make predictions using the best model\n#y_pred_best_svm = best_svm_clf.predict(X_test)\n\n# Evaluate the best model\n#accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)\n#classification_rep_best_svm = classification_report(y_test, y_pred_best_svm)\n\n#best_params, accuracy_best_svm, classification_rep_best_svm\n\n\n# Initialize the Support Vector Machine classifier\n#svm_clf = SVC(kernel='linear', gamma = 'scale', C = 0.1, random_state=42)\n\n# Fit the model on the training data\n#svm_clf.fit(X_train, y_train)\n\n\n# Using pickle\n#with open('svm_model1.pkl', 'wb') as model_file:\n#    pickle.dump(svm_clf, model_file)\n\n# Using pickle\nwith open('svm_model1.pkl', 'rb') as model_file:\n    loaded_svm_pickle1 = pickle.load(model_file)\n\n# Make predictions on the testing data\ny_pred_svm = loaded_svm_pickle1.predict(X_test)\n\n# Evaluate the classifier\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nclassification_rep_svm = classification_report(y_test, y_pred_svm)\n\n# Print the accuracy and classification report\nprint(f'test score {accuracy_svm}')\n\ntest score 0.5935412026726058"
  },
  {
    "objectID": "index.html#catboost",
    "href": "index.html#catboost",
    "title": "SONA_NLP_Python",
    "section": "Catboost",
    "text": "Catboost\n55.68\n\n# Initialize LabelEncoder\nle = LabelEncoder()\n\n# Fit and transform the training data\ny_train_encoded = le.fit_transform(y_train)\n\n# Transform the testing data\ny_test_encoded = le.transform(y_test)\n\n# Create pool with training data\npool = Pool(data=X_train, label=y_train_encoded, cat_features=[])\n\n# Parameters for CatBoostClassifier\nparams = {\n    'iterations': 3000,\n    'depth': 5,\n    'learning_rate': 0.05,\n    'loss_function': 'MultiClass',\n    'random_seed': 42,\n    'verbose': 10\n}\n\n# Perform cross-validation and save results\n#cv_results = cv(\n#    pool=pool,\n#    params=params,\n#    fold_count=5,\n#    plot=False,\n#    early_stopping_rounds=10\n#)\n\n# Save cross-validation results to a CSV file\n#cv_results.to_csv('cv_results_over_epochs_2.csv', index=False)\n\n# Train model on full training set and save it\n#clf = CatBoostClassifier(**params)\n#clf.fit(pool)\n#clf.save_model('catboost_TFIDF.cbm')\n\n# Load model from file\nloaded_model = CatBoostClassifier()\nloaded_model.load_model('catboost_TFIDF.cbm')\n\n# Make predictions on testing data\ny_pred = loaded_model.predict(X_test)\ny_pred_class = y_pred.flatten().astype(int)\n\n# Inverse transform the predictions back to original labels\ny_pred_decoded = le.inverse_transform(y_pred_class)\n\n# Evaluate the classifier using the original (non-encoded) labels\naccuracy = accuracy_score(y_test_encoded, y_pred_class)\nclassification_rep = classification_report(y_test_encoded, y_pred_class, target_names=le.classes_)\n\n# Print accuracy and classification report\nprint(f'Accuracy: {accuracy * 100:.2f}%')\nprint('Classification Report:')\nprint(classification_rep)\n\nAccuracy: 55.68%\nClassification Report:\n              precision    recall  f1-score   support\n\n     Mandela       0.55      0.29      0.38       334\n       Mbeki       0.55      0.59      0.57       480\n   Ramaphosa       0.58      0.57      0.58       456\n        Zuma       0.55      0.68      0.61       526\n\n    accuracy                           0.56      1796\n   macro avg       0.56      0.53      0.53      1796\nweighted avg       0.56      0.56      0.55      1796\n\n\n\n\n# Read the CSV file\nnew_df = pd.read_csv(\"cv_results_over_epochs_2.csv\")\n\n# Plotting the mean values for training and validation errors with distinct colors\nplt.plot(new_df['iterations'], new_df['train-MultiClass-mean'], label='Training Error', color='b')  \nplt.plot(new_df['iterations'], new_df['test-MultiClass-mean'], label='Validation Error', color='r')  \n\n# Adding title and labels to the plot\nplt.title('Training and Validation Error Over Epochs')\nplt.xlabel('Epochs (Iterations)')\nplt.ylabel('Error')\n\n# Adding legend to the plot\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "index.html#svm-1",
    "href": "index.html#svm-1",
    "title": "SONA_NLP_Python",
    "section": "SVM",
    "text": "SVM\ngrid search SVM\nbest params: 10, ‘gamma’: ‘scale’, ‘kernel’: ‘rbf’\n\n#param_grid = {\n#    'C': [0.1, 1, 10],  # Regularization parameter\n#    'kernel': ['linear', 'rbf'],  # Type of SVM\n#    'gamma': ['scale', 'auto']  # Kernel coefficient\n#}\n\n# Initialize the SVM classifier\n#svm = SVC(random_state=42)\n\n# Initialize Grid Search\n#grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy')\n\n# Perform Grid Search on the training data\n#grid_search.fit(X_train, y_train)\n\n# Get the best parameters and the best estimator from Grid Search\n#best_params = grid_search.best_params_\n#best_svm_clf = grid_search.best_estimator_\n\n# Make predictions using the best model\n#y_pred_best_svm = best_svm_clf.predict(X_test)\n\n# Evaluate the best model\n#accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)\n#classification_rep_best_svm = classification_report(y_test, y_pred_best_svm)\n\n#best_params, accuracy_best_svm, classification_rep_best_svm\n\n\n# Initialize the Support Vector Machine classifier\n#svm_clf = SVC(kernel='rbf', gamma = 'scale', C = 10, random_state=42)\n\n# Fit the model on the training data\n#svm_clf.fit(X_train, y_train)\n\n#with open('svm_model2.pkl', 'wb') as model_file:\n#    pickle.dump(svm_clf, model_file)\n\n# Using pickle\nwith open('svm_model2.pkl', 'rb') as model_file:\n    loaded_svm2_pickle = pickle.load(model_file)\n\n# Make predictions on the testing data\ny_pred_svm = loaded_svm2_pickle.predict(X_test)\n\n# Evaluate the classifier\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nclassification_rep_svm = classification_report(y_test, y_pred_svm)\n\n# Print the accuracy and classification report\naccuracy_svm\n\n0.6146993318485523\n\n\n\nNeural Net\n\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# Convert the sparse matrix to dense matrix as neural network needs dense input\nX_train_dense = X_train.toarray()\nX_test_dense = X_test.toarray()\n\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train)\n#seed = 7\n#np.random.seed(seed)\n\n# Define 5-fold cross-validation\n#num_folds = 5\n#kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n\n# Initialize variables to store sum of accuracies for each epoch\n#sum_train_accuracy = []\n#sum_val_accuracy = []\n\n#for train, test in kfold.split(X_train_dense, y_train):\n    # Split data into train and validation sets for this fold\n#    X_train_fold, X_val_fold = X_train_dense[train], X_train_dense[test]\n#    y_train_fold, y_val_fold = y_train[train], y_train[test]\n\n#    model = Sequential([\n#        Dense(505, input_dim=X_train_fold.shape[1], activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-8)),\n#        Dense(220, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-7)),\n#        Dense(len(np.unique(y_train)), activation='softmax')\n#    ])\n    \n#    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Train model and store training history\n#    history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=20)\n\n    # Append accuracies for each epoch for this fold\n#    sum_train_accuracy.append(history.history['accuracy'])\n#    sum_val_accuracy.append(history.history['val_accuracy'])\n\n# Compute average accuracies for each epoch\n#avg_train_accuracy = np.mean(sum_train_accuracy, axis=0)\n#avg_val_accuracy = np.mean(sum_val_accuracy, axis=0)\n\n# Save averaged accuracies to CSV\n#avg_accuracies_df = pd.DataFrame({'avg_train_accuracy': avg_train_accuracy, 'avg_val_accuracy': avg_val_accuracy})\n#avg_accuracies_df.to_csv('avg_accuracies_over_epochs_NN_2.csv', index=False)\n\n# Load averaged accuracies from CSV\nloaded_avg_accuracies = pd.read_csv('avg_accuracies_over_epochs_NN_2.csv')\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(range(1, len(loaded_avg_accuracies['avg_train_accuracy']) + 1), loaded_avg_accuracies['avg_train_accuracy'], label='Average Training Accuracy')\nplt.plot(range(1, len(loaded_avg_accuracies['avg_val_accuracy']) + 1), loaded_avg_accuracies['avg_val_accuracy'], label='Average Validation Accuracy', linestyle='dashed')\n\nplt.title('Average Training and Validation Accuracy Over Epochs')\nplt.xlabel('Epoch')\nplt.xticks(range(1, len(loaded_avg_accuracies['avg_train_accuracy']) + 1))  # Adjust x-axis ticks to start from 1\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\nTest Accuracy: 59.58%\n\n#X_train = X_train.toarray()\n#X_test = X_test.toarray()\nencoder = LabelEncoder()\ny_test = encoder.fit_transform(y_test)\n# Define model\n#model = Sequential([\n#    Dense(505, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-8)),  # Input layer\n#    Dense(220, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-7)),  # Hidden layer\n#    Dense(len(label_mapping), activation='softmax')  # Output layer\n#])\n\n# Compile model\n#model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\n#epochs = 2\n#model.fit(X_train, y_train, validation_split=0.2, epochs=epochs)\n\n# Save the final model\n#model.save('NN_final_model_2.h5')\n\n# Load the final model\nloaded_model = load_model('NN_final_model_2.h5')\n\n\n# Evaluate the loaded model on the test set\nfinal_loss, final_accuracy = loaded_model.evaluate(X_test, y_test)\nprint(f'Test Loss: {final_loss:.4f}')\nprint(f'Test Accuracy: {final_accuracy*100:.2f}%')\n\n 1/57 [..............................] - ETA: 7s - loss: 1.2242 - accuracy: 0.5625\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/57 [===>..........................] - ETA: 0s - loss: 0.9624 - accuracy: 0.6250\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/57 [======>.......................] - ETA: 0s - loss: 0.9991 - accuracy: 0.6167\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/57 [==========>...................] - ETA: 0s - loss: 0.9943 - accuracy: 0.6207\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/57 [==============>...............] - ETA: 0s - loss: 1.0090 - accuracy: 0.6045\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/57 [=================>............] - ETA: 0s - loss: 1.0180 - accuracy: 0.6076\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43/57 [=====================>........] - ETA: 0s - loss: 1.0540 - accuracy: 0.5930\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/57 [=========================>....] - ETA: 0s - loss: 1.0437 - accuracy: 0.5975\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/57 [==============================] - ETA: 0s - loss: 1.0468 - accuracy: 0.5958\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/57 [==============================] - 1s 8ms/step - loss: 1.0468 - accuracy: 0.5958\n\n\nTest Loss: 1.0468\nTest Accuracy: 59.58%"
  },
  {
    "objectID": "index.html#catboost-1",
    "href": "index.html#catboost-1",
    "title": "SONA_NLP_Python",
    "section": "Catboost",
    "text": "Catboost\nAccuracy: 29.18%\n\npool = Pool(data=X_train, label=y_train, cat_features=[])\n# Parameters for the CatBoostClassifier\nparams = {\n    'iterations': 3000,          # Number of boosting iterations\n    'depth': 5,                # Depth of the trees\n    'learning_rate': 0.05,      # Learning rate\n    'loss_function': 'MultiClass',  # Objective function\n    'random_seed': 42,             # Random seed\n    'verbose': 10                   # Output training information every 10 iterations\n}\n\n#cv_results = cv(\n #   pool=pool,\n #   params=params,\n #   fold_count=5,  # Number of folds in CV\n #   plot=False,   # Set to True if you want to see the plot of train and test errors during cross-validation\n  #  early_stopping_rounds=10\n#)\n\n# Save cross-validation results to a CSV file\n#cv_results.to_csv('cv_results_over_epochs_3.csv', index=False)\n\n# Train the model on the full training set and save it\n#clf = CatBoostClassifier(**params)\n#clf.fit(pool)\n#clf.save_model('catboost_WordEmbed.cbm')\n\n# Load the model from the file\nloaded_model = CatBoostClassifier()\nloaded_model.load_model('catboost_WordEmbed.cbm')\n\n# Make predictions on the testing data\ny_pred = loaded_model.predict(X_test)\ny_pred_decoded = le.inverse_transform(y_pred.flatten().astype(int))\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred.flatten().astype(int))\nclassification_rep = classification_report(y_test, y_pred.flatten().astype(int), target_names=le.classes_)\n\n# Print the accuracy and classification report\nprint(f'Accuracy: {accuracy * 100:.2f}%')\nprint('Classification Report:')\nprint(classification_rep)\n\nAccuracy: 29.12%\nClassification Report:\n              precision    recall  f1-score   support\n\n     Mandela       0.00      0.00      0.00       334\n       Mbeki       0.00      0.00      0.00       480\n   Ramaphosa       0.30      0.96      0.45       526\n        Zuma       0.19      0.04      0.06       456\n\n    accuracy                           0.29      1796\n   macro avg       0.12      0.25      0.13      1796\nweighted avg       0.13      0.29      0.15      1796\n\n\n\n/opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning:\n\nPrecision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n/opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning:\n\nPrecision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n/opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning:\n\nPrecision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n\n\n\n# Read the CSV file\nnew_df = pd.read_csv(\"cv_results_over_epochs_3.csv\")\n\n# Plotting the mean values for training and validation errors with distinct colors\nplt.plot(new_df['iterations'], new_df['train-MultiClass-mean'], label='Training Error', color='b')  \nplt.plot(new_df['iterations'], new_df['test-MultiClass-mean'], label='Validation Error', color='r')  \n\n# Adding title and labels to the plot\nplt.title('Training and Validation Error Over Epochs')\nplt.xlabel('Epochs (Iterations)')\nplt.ylabel('Error')\n\n# Adding legend to the plot\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "index.html#svm-2",
    "href": "index.html#svm-2",
    "title": "SONA_NLP_Python",
    "section": "SVM",
    "text": "SVM\n\n#param_grid = {\n#    'C': [0.1, 1, 10],  # Regularization parameter\n#    'kernel': ['linear', 'rbf'],  # Type of SVM\n#    'gamma': ['scale', 'auto']  # Kernel coefficient\n#}\n\n# Initialize the SVM classifier\n#svm = SVC(random_state=42)\n\n# Initialize Grid Search\n#grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy')\n\n# Perform Grid Search on the training data\n#grid_search.fit(X_train, y_train)\n\n# Get the best parameters and the best estimator from Grid Search\n#best_params = grid_search.best_params_\n#best_svm_clf = grid_search.best_estimator_\n\n# Make predictions using the best model\n#y_pred_best_svm = best_svm_clf.predict(X_test)\n\n# Evaluate the best model\n#accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)\n#classification_rep_best_svm = classification_report(y_test, y_pred_best_svm, target_names=label_mapping.keys())\n\n41.26%\n\n# Initialize the Support Vector Machine classifier with specific parameters\n#specific_svm_clf = SVC(kernel='linear', gamma='scale', C=10, random_state=42)\n\n# Fit the model on the training data\n#specific_svm_clf.fit(X_train, y_train)\n\n#with open('svm_model3.pkl', 'wb') as model_file:\n#    pickle.dump(specific_svm_clf, model_file)\n\n# Using pickle\nwith open('svm_model3.pkl', 'rb') as model_file:\n    loaded_svm3_pickle = pickle.load(model_file)\n\n# Make predictions on the testing data\ny_pred_specific_svm = loaded_svm3_pickle.predict(X_test)\n\n\n\n# Evaluate the classifier\naccuracy_specific_svm = accuracy_score(y_test, y_pred_specific_svm)\nclassification_rep_specific_svm = classification_report(y_test, y_pred_specific_svm, target_names=label_mapping.keys())\n\n# Print the accuracy and classification report\nprint(f'Accuracy (Specific SVM): {accuracy_specific_svm * 100:.2f}%')\nprint('Classification Report (Specific SVM):')\nprint(classification_rep_specific_svm)\n\nAccuracy (Specific SVM): 28.01%\nClassification Report (Specific SVM):\n              precision    recall  f1-score   support\n\n     Mandela       0.22      0.67      0.34       334\n       Mbeki       0.00      0.00      0.00       480\n        Zuma       0.61      0.08      0.14       526\n   Ramaphosa       0.32      0.52      0.40       456\n\n    accuracy                           0.28      1796\n   macro avg       0.29      0.32      0.22      1796\nweighted avg       0.30      0.28      0.21      1796\n\n\n\n/opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning:\n\nPrecision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n/opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning:\n\nPrecision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n/opt/anaconda3/envs/firstEnv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning:\n\nPrecision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior."
  },
  {
    "objectID": "index.html#neural-network-1",
    "href": "index.html#neural-network-1",
    "title": "SONA_NLP_Python",
    "section": "Neural Network",
    "text": "Neural Network\nAccuracy: 36.69%\n\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# Convert the sparse matrix to dense matrix as neural network needs dense input\n#X_train_dense = X_train.toarray()\n#X_test_dense = X_test.toarray()\n\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train)\n#seed = 7\n#np.random.seed(seed)\n\n# Define 5-fold cross-validation\n#num_folds = 5\n#kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n\n# Initialize variables to store sum of accuracies for each epoch\n#sum_train_accuracy = []\n#sum_val_accuracy = []\n\n#for train, test in kfold.split(X_train_dense, y_train):\n    # Split data into train and validation sets for this fold\n #   X_train_fold, X_val_fold = X_train_dense[train], X_train_dense[test]\n #   y_train_fold, y_val_fold = y_train[train], y_train[test]\n\n #   model = Sequential([\n #       Dense(505, input_dim=X_train_fold.shape[1], activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-8)),\n #       Dense(220, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-7)),\n #       Dense(len(np.unique(y_train)), activation='softmax')\n #   ])\n    \n#    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Train model and store training history\n #   history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=20)\n\n    # Append accuracies for each epoch for this fold\n #   sum_train_accuracy.append(history.history['accuracy'])\n #   sum_val_accuracy.append(history.history['val_accuracy'])\n\n# Compute average accuracies for each epoch\n#avg_train_accuracy = np.mean(sum_train_accuracy, axis=0)\n#avg_val_accuracy = np.mean(sum_val_accuracy, axis=0)\n\n# Save averaged accuracies to CSV\n#avg_accuracies_df = pd.DataFrame({'avg_train_accuracy': avg_train_accuracy, 'avg_val_accuracy': avg_val_accuracy})\n#avg_accuracies_df.to_csv('avg_accuracies_over_epochs_NN_3.csv', index=False)\n\n# Load averaged accuracies from CSV\nloaded_avg_accuracies = pd.read_csv('avg_accuracies_over_epochs_NN_3.csv')\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(range(1, len(loaded_avg_accuracies['avg_train_accuracy']) + 1), loaded_avg_accuracies['avg_train_accuracy'], label='Average Training Accuracy')\nplt.plot(range(1, len(loaded_avg_accuracies['avg_val_accuracy']) + 1), loaded_avg_accuracies['avg_val_accuracy'], label='Average Validation Accuracy', linestyle='dashed')\n\nplt.title('Average Training and Validation Accuracy Over Epochs')\nplt.xlabel('Epoch')\nplt.xticks(range(1, len(loaded_avg_accuracies['avg_train_accuracy']) + 1))  # Adjust x-axis ticks to start from 1\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\nTest Accuracy: 36.08%\n\n#X_train = X_train.toarray()\n#X_test = X_test.toarray()\nencoder = LabelEncoder()\ny_test = encoder.fit_transform(y_test)\n# Define model\n#model = Sequential([\n #   Dense(505, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-8)),  # Input layer\n #   Dense(220, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(1e-7)),  # Hidden layer\n  #  Dense(len(label_mapping), activation='softmax')  # Output layer\n#])\n\n# Compile model\n#model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\n#epochs = 3\n#model.fit(X_train, y_train, validation_split=0.2, epochs=epochs)\n\n# Save the final model\n#model.save('NN_final_model_3.h5')\n\n# Load the final model\nloaded_model = load_model('NN_final_model_3.h5')\n\n\n# Evaluate the loaded model on the test set\nfinal_loss, final_accuracy = loaded_model.evaluate(X_test, y_test)\nprint(f'Test Loss: {final_loss:.4f}')\nprint(f'Test Accuracy: {final_accuracy*100:.2f}%')\n\n 1/57 [..............................] - ETA: 6s - loss: 1.2420 - accuracy: 0.4375\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42/57 [=====================>........] - ETA: 0s - loss: 1.4058 - accuracy: 0.3103\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/57 [==============================] - 0s 1ms/step - loss: 1.4071 - accuracy: 0.3096\n\n\nTest Loss: 1.4071\nTest Accuracy: 30.96%"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]