<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>SONA_NLP_Python</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
        <script type="text/javascript">
        window.PlotlyConfig = {MathJaxConfig: 'local'};
        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
        if (typeof require !== 'undefined') {
        require.undef("plotly");
        requirejs.config({
            paths: {
                'plotly': ['https://cdn.plot.ly/plotly-2.26.0.min']
            }
        });
        require(['plotly'], function(Plotly) {
            window._Plotly = Plotly;
        });
        }
        </script>
        

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">SONA_NLP_Python</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Bibliography.html">
 <span class="menu-text">Bibliography</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Appendix.html">
 <span class="menu-text">Appendix</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#literature-review" id="toc-literature-review" class="nav-link" data-scroll-target="#literature-review">Literature Review</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#data-pre-processing" id="toc-data-pre-processing" class="nav-link" data-scroll-target="#data-pre-processing">Data Pre-Processing</a></li>
  <li><a href="#feature-extraction" id="toc-feature-extraction" class="nav-link" data-scroll-target="#feature-extraction">Feature Extraction</a>
  <ul class="collapse">
  <li><a href="#bag-of-words-bow" id="toc-bag-of-words-bow" class="nav-link" data-scroll-target="#bag-of-words-bow">Bag of Words (BoW)</a></li>
  <li><a href="#term-frequency-inverse-document-frequency-tf-idf" id="toc-term-frequency-inverse-document-frequency-tf-idf" class="nav-link" data-scroll-target="#term-frequency-inverse-document-frequency-tf-idf">Term Frequency-Inverse Document Frequency (TF-IDF)</a></li>
  <li><a href="#word-embeddings" id="toc-word-embeddings" class="nav-link" data-scroll-target="#word-embeddings">Word Embeddings</a></li>
  </ul></li>
  <li><a href="#modelling" id="toc-modelling" class="nav-link" data-scroll-target="#modelling">Modelling</a>
  <ul class="collapse">
  <li><a href="#gradient-boosted-trees" id="toc-gradient-boosted-trees" class="nav-link" data-scroll-target="#gradient-boosted-trees">Gradient Boosted Trees</a></li>
  <li><a href="#support-vector-machines" id="toc-support-vector-machines" class="nav-link" data-scroll-target="#support-vector-machines">Support Vector Machines</a></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks">Neural Networks</a></li>
  </ul></li>
  <li><a href="#model-evaluation-metrics" id="toc-model-evaluation-metrics" class="nav-link" data-scroll-target="#model-evaluation-metrics">Model Evaluation Metrics</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#data-exploration-results" id="toc-data-exploration-results" class="nav-link" data-scroll-target="#data-exploration-results">Data Exploration Results</a></li>
  <li><a href="#model-evaluation" id="toc-model-evaluation" class="nav-link" data-scroll-target="#model-evaluation">Model Evaluation</a>
  <ul class="collapse">
  <li><a href="#accuracy" id="toc-accuracy" class="nav-link" data-scroll-target="#accuracy">Accuracy</a></li>
  <li><a href="#f1-score" id="toc-f1-score" class="nav-link" data-scroll-target="#f1-score">F1 Score</a></li>
  </ul></li>
  <li><a href="#bag-of-words" id="toc-bag-of-words" class="nav-link" data-scroll-target="#bag-of-words">Bag of Words</a>
  <ul class="collapse">
  <li><a href="#boosted-trees" id="toc-boosted-trees" class="nav-link" data-scroll-target="#boosted-trees">Boosted Trees</a></li>
  <li><a href="#neural-network" id="toc-neural-network" class="nav-link" data-scroll-target="#neural-network">Neural Network</a></li>
  <li><a href="#svms" id="toc-svms" class="nav-link" data-scroll-target="#svms">SVMs</a></li>
  </ul></li>
  <li><a href="#tf-idf" id="toc-tf-idf" class="nav-link" data-scroll-target="#tf-idf">TF-IDF</a>
  <ul class="collapse">
  <li><a href="#boosted-trees-1" id="toc-boosted-trees-1" class="nav-link" data-scroll-target="#boosted-trees-1">Boosted Trees</a></li>
  <li><a href="#svm" id="toc-svm" class="nav-link" data-scroll-target="#svm">SVM</a></li>
  <li><a href="#neural-net" id="toc-neural-net" class="nav-link" data-scroll-target="#neural-net">Neural Net</a></li>
  </ul></li>
  <li><a href="#word-embeddings-1" id="toc-word-embeddings-1" class="nav-link" data-scroll-target="#word-embeddings-1">Word Embeddings</a>
  <ul class="collapse">
  <li><a href="#catboost" id="toc-catboost" class="nav-link" data-scroll-target="#catboost">Catboost</a></li>
  <li><a href="#svm-1" id="toc-svm-1" class="nav-link" data-scroll-target="#svm-1">SVM</a></li>
  <li><a href="#neural-network-1" id="toc-neural-network-1" class="nav-link" data-scroll-target="#neural-network-1">Neural Network</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">SONA_NLP_Python</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>This paper critically analyzes the State of the Nation Address (SONA) speeches delivered by various South African presidents from 1994 to 2023. The primary objective is to categorize each president based on single sentences extracted from their respective SONA speeches. The study unfolds as follows: Initially, a concise literature review is presented, with emphasis on the domain of Natural Language Processing (NLP), particularly focusing on classification tasks within NLP. This review lays the groundwork for the methodologies and approaches applied in later sections of the paper. Subsequent sections offer an in-depth exploration and meticulous cleaning of the data utilized in the study. The exploration phase scrutinizes the dataset’s balance and analyzes the vocabulary used, both overall and by each specific president. These preliminary steps are crucial for ensuring the integrity and reliability of the study’s findings. The paper then transitions to a detailed exposition of the methodologies employed in the study. The methods section elucidates the three feature extraction tools deployed: Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Word Tokenization. Additionally, it describes the three predictive models applied, namely Gradient Boosted Trees, Feed Forward Neural Networks, and Support Vector Machines. Each tool and model is presented with a rationale for its inclusion and an explanation of its contribution to the study’s objectives.</p>
<p>Following the methods section, the paper presents and succinctly discusses the study’s results. This section provides an initial interpretation of the findings, preparing the ground for the more in-depth analysis that follows. In the penultimate section, a comprehensive discussion of the results is provided. This discussion delves into the insights gleaned from the findings, offering detailed interpretations and drawing connections with the literature reviewed earlier. This section aims not only to shed light on the study’s findings but also to locate these within the broader academic discourse on the subject. Finally, the paper concludes with a reflective overview of the study as a whole. This concluding section evaluates the study’s successes and limitations, reflects on its contributions to the field, and suggests avenues for future research and exploration. Through this reflective lens, the paper not only summarizes its findings but also invites further scholarly engagement with the questions and challenges raised during the study.</p>
</section>
<section id="literature-review" class="level1">
<h1>Literature Review</h1>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<p>The methods applied fall into four main categories that also follow the workflow of the project. More specifically, data pre-processing, feature extraction, modelling and model evaluation.</p>
<section id="data-pre-processing" class="level2">
<h2 class="anchored" data-anchor-id="data-pre-processing">Data Pre-Processing</h2>
<p>In the data preprocessing phase before feature extraction, initial data loading was accomplished from text files, with each file containing SONA speeches from different South African presidents from 1994 to 2023. Files were filtered to ensure they were valid, and the president’s names were extracted and cleaned for later use. The speeches within each file were tokenized into sentences using the NLTK library, and any unnecessary newline characters within these sentences were removed. Each sentence was then associated with the relevant president, resulting in a structured data frame containing each sentence alongside its corresponding president’s name. Following this, the data underwent exploratory data analysis (EDA) where sentences associated with specific presidents were filtered out, and sentence lengths were calculated and visualized. The sentences were further cleaned by removing stop words (common words that do not contribute to the meaning of a sentence) and then grouped by president. Subsequently, the cleaned sentences were used for generating word clouds for visual inspection. Furthermore, the most common words across all presidents were calculated and the average words per sentence were computed and plotted for each president.</p>
</section>
<section id="feature-extraction" class="level2">
<h2 class="anchored" data-anchor-id="feature-extraction">Feature Extraction</h2>
<section id="bag-of-words-bow" class="level3">
<h3 class="anchored" data-anchor-id="bag-of-words-bow">Bag of Words (BoW)</h3>
<p>The Bag of Words (BoW) method represents text data as a matrix of token (typically words) occurrences within a given document. Each row of the matrix corresponds to a document, while each column represents a unique token in the dataset. The matrix cell contains the count of occurrences of the token in the document. In mathematical terms, for a set of <span class="math inline">\(n\)</span> documents <span class="math inline">\(D\)</span> and a set of <span class="math inline">\(m\)</span> unique tokens <span class="math inline">\(T\)</span>, the BoW matrix <span class="math inline">\(M\)</span> is a <span class="math inline">\(n \times m\)</span> matrix where <span class="math inline">\(M_{ij}\)</span> is the frequency of token <span class="math inline">\(j\)</span> in document <span class="math inline">\(i\)</span>(Akuma et al., 2022). For the dataset in question, each sentence from the president’s speeches is treated as a document. The BoW model tokenizes each sentence into words, creating a matrix that reflects the frequency count of each word within each sentence, yielding a sparse matrix representation of the word distribution in each president’s speech.</p>
</section>
<section id="term-frequency-inverse-document-frequency-tf-idf" class="level3">
<h3 class="anchored" data-anchor-id="term-frequency-inverse-document-frequency-tf-idf">Term Frequency-Inverse Document Frequency (TF-IDF)</h3>
<p>The Term Frequency-Inverse Document Frequency (TF-IDF) technique assigns a weight to each term in a document reflecting its importance in the document relative to the entire corpus. The TF-IDF value of a term <span class="math inline">\(t\)</span> in a document <span class="math inline">\(d\)</span> within a corpus <span class="math inline">\(D\)</span> is computed as <span class="math inline">\(\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t, D)\)</span>, where <span class="math inline">\(\text{TF}(t, d)\)</span> is the frequency of term <span class="math inline">\(t\)</span> in document <span class="math inline">\(d\)</span> divided by the total number of terms in <span class="math inline">\(d\)</span>, and <span class="math inline">\(\text{IDF}(t, D)\)</span> is the logarithm of the total number of documents in <span class="math inline">\(D\)</span> divided by the number of documents containing term <span class="math inline">\(t\)</span>(Akuma et al., 2022). In the context of the dataset, TF-IDF is calculated for each term in every sentence, resulting in a vector of TF-IDF values for each sentence, thereby emphasizing terms that are distinctive to specific speeches or presidents.</p>
</section>
<section id="word-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="word-embeddings">Word Embeddings</h3>
<p>Sentences from the presidential speeches dataset are tokenized into words. These tokens are then fed into the Word2Vec model, which learns vector representations for each word by predicting the context in which a word appears, effectively capturing the semantic relationships between words(Řehůřek, 2022). Upon training Word2Vec with the tokenized sentences, each word is represented as a high-dimensional vector. To form a representative vector for a complete sentence, the word vectors within each sentence are averaged. This results in a single vector per sentence, encapsulating the semantic essence of the sentence based on its constituent words.</p>
<p>These sentence-level vectors serve as the dataset’s numerical features, providing a semantically rich representation of the sentences for subsequent machine-learning applications in the project. Each vector not only represents its sentence but also mirrors the inherent semantic structure and relationships within the text, offering a meaningful feature set for analysis( Řehůřek, 2022). By employing these methods, the raw textual data from the speeches is transformed into a numerical format suitable for training machine learning models.</p>
</section>
</section>
<section id="modelling" class="level2">
<h2 class="anchored" data-anchor-id="modelling">Modelling</h2>
<p>Note that for each of the following predictive models, each of the three feature extraction methods detailed above are applied.</p>
<section id="gradient-boosted-trees" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosted-trees">Gradient Boosted Trees</h3>
<p>Gradient Boosting is a general technique where models are built sequentially, with each new model being trained to correct the mistakes of the combined ensemble of existing models. This process is iteratively repeated, progressively improving the model’s accuracy until further improvements are negligible(Brownlee, 2021).</p>
<p>Gradient Boosted Trees (GBTs) inherently leverage an ensemble methodology, combining the predictive power of multiple weak learners, in decision trees, to create a more accurate and robust model. The algorithm incrementally builds an ensemble of trees where each subsequent tree compensates for the errors of the aggregate set of preceding trees(Friedman, 2001). Through this iterative refinement, the algorithm not only enhances its precision but also avoids overfitting, providing a reliable generalization to unseen data.</p>
<p>In our deployment of GBTs, using Yandex’s CatBoost, careful parameter tuning was essential for optimized performance. We engaged 500 boosting iterations, a 0.05 learning rate, and a tree depth of 10. The boosting iterations define the number of trees in the model, with each iteration adding a new tree that corrects the errors of the ensemble. The learning rate, alternatively known as shrinkage, moderates the influence of each tree, preventing any single tree from dominating the ensemble prediction. The tree depth, meanwhile, influences the model’s complexity, with deeper trees allowing for the capture of more complex patterns in the data but at the risk of overfitting. These parameters were selected after running a grid search over hyperparameters choosing the hyperamters which minimised validation error computed using 5-fold cross-validation.</p>
</section>
<section id="support-vector-machines" class="level3">
<h3 class="anchored" data-anchor-id="support-vector-machines">Support Vector Machines</h3>
<p>Support Vector Machines (SVMs) is a supervised learning algorithm used for classification tasks. SVMs operate by constructing hyperplanes in a multidimensional space that separates cases of different class labels(Shmilovici, 2005). The SVM algorithm is implemented in practice using a kernel. The kernel trick helps in fitting the maximum-margin hyperplane in the transformed feature space. The hyperplane is selected to segregate classes in the best possible way.</p>
<p>In this project, SVM classifiers were employed with various parameter configurations, and their performance was evaluated based on the processed data obtained through the feature extraction methods previously mentioned. A grid search approach was utilized to explore a range of parameter values and identify the optimal configuration for each data representation method. The grid search was performed over a range of values for the <code>C</code> parameter, different kernel types, and the gamma coefficient for the Radial Basis Function (RBF) kernel. Specifically, the <code>C</code> parameter was explored over the values <code>[0.1, 1, 10]</code>. The <code>C</code> parameter represents the regularization term, controlling the trade-off between having a smooth decision boundary and classifying the training points correctly(Sklearn, 2023).</p>
<p>The kernel parameter, with values <code>['linear', 'rbf']</code>, specifies the type of hyperplane used to separate the data. The linear kernel is often used when the data is linearly separable, meaning it can be separated by a single line(Sklearn, 2023). The RBF kernel is used for non-linear data, transforming it into a higher-dimensional space where it becomes linearly separable. The gamma parameter, with values <code>['scale', 'auto']</code>, defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’(Sklearn, 2023).</p>
<p>For the SVM model trained on data represented using Bag of Words, the grid search optimal parameters were <code>C=0.1</code>, a <code>linear</code> kernel and <code>gamma='scale'</code>. When using TF-IDF, the optimal parameters were <code>C=10</code>, a <code>RBF</code> kernel with <code>gamma='scale'</code>. Finally, for Word Embeddings, a <code>C</code> value of <code>10</code>, a <code>linear</code> kernel, and <code>gamma='scale'</code>.</p>
</section>
<section id="neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="neural-networks">Neural Networks</h3>
<p>The Neural Network (NN) model deployed in this project is a multilayer perceptron (MLP), which is a type of feedforward neural network(Goodfellow et al., 2016). The input data was converted from a sparse to a dense matrix format. The architecture employed was the same across all feature extraction methods as varying the model architecture did not prove to yield any significant difference in model performance across the feature extraction methods.</p>
<p>The architecture of the NN consists of an input layer, a hidden layer, and an output layer. The input layer has 505 neurons, with a Rectified Linear Unit (ReLU) as the activation function. ReLU was chosen due to its effectiveness in mitigating the vanishing gradient problem(Glorot et al., 2011). For the weight initialization for the input layer, the He normal initializer was used. This initializer assists in breaking symmetry and facilitating the learning process(), and ensures a less-naive starting point for the weights(He et al., 2015). Additionally, to avoid overfitting, L2 regularization with a coefficient of <span class="math inline">\(1 \times 10^{-8}\)</span> was applied.</p>
<p>The hidden layer also uses a ReLU activation function and the He normal initializer, with 220 neurons. L2 regularization using <span class="math inline">\(1 \times 10^{-7}\)</span> was incorporated. The softmax activation function is used in the final layer to output a probability distribution over the multiple classes.</p>
<p>During the training phase, a stratified 5-fold cross-validation strategy was employed. For each fold, the data was split into training and validation subsets, with the model being trained for 20 epochs. The sparse categorical cross-entropy was selected as the loss function, being appropriate for multi-class classification problems. The Adam optimizer was utilized for its adaptive learning rate capabilities. Following examination of the plot of training versus validation accuracy over epochs, the optimal number of epochs for training was set, and the final model was trained and subsequently evaluated on the unseen test data.</p>
</section>
</section>
<section id="model-evaluation-metrics" class="level2">
<h2 class="anchored" data-anchor-id="model-evaluation-metrics">Model Evaluation Metrics</h2>
<p>The evaluation of the models’ performance employed a suite of metrics to ensure a comprehensive assessment. The primary metrics used are accuracy and F1-score. Accuracy is defined as the ratio of correctly predicted observations to the total observations:</p>
<p><span class="math inline">\(\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions Made}}\)</span></p>
<p>While accuracy is a straightforward and informative metric, it is not sufficient where the classes are imbalanced. Hence, additional evaluative metrics were incorporated into the assessment framework. Stratified k-fold cross-validation (with <span class="math inline">\(k = 5\)</span> ) was used during the training phase of Neural Networks (NNs).</p>
<p>Subsequently, confusion matrices were generated for the models. A confusion matrix is a table used to describe the performance of a classification algorithm. Each row of the matrix represents the instances of the actual class, while each column represents the instances of the predicted class. The entries on the main diagonal of the confusion matrix correspond to correct predictions, whereas other entries reflect the mistakes made by the classifier. Through the confusion matrix, the F1-score was computed to provide a detailed understanding of the model’s performance, F1 score is composed of two other metrics in precision and recall. Precision (the ratio of true positive predictions to the sum of true positive and false positive predictions), and recall (the ratio of true positive predictions to the sum of true positive and false negative predictions), are both utilized to form the F1-score (the harmonic mean of precision and recall).</p>
<p>For the Gradient Boosted Trees model, the loss function was an essential aspect of the evaluation. The loss function is a measure used to estimate the discrepancy between the predicted values and actual values. During the training process, the algorithm optimizes the model parameters to minimize this loss function, thereby improving the model’s predictive performance. Monitoring the change in the value of the loss function during the training process provides insights into the convergence and stability of the model, helping in understanding whether the model is learning effectively from the training data. For each training epoch, the loss was computed and analyzed to ensure the model was converging to a solution that minimized prediction errors.</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<section id="data-exploration-results" class="level2">
<h2 class="anchored" data-anchor-id="data-exploration-results">Data Exploration Results</h2>
<p>Prior to investigating the models, a preliminary data exploration was conducted to understand the data better. We begin first by plotting the number of sentences per president to understand the balance of the dataset shown in Figure 1. The bar plot displays the distribution of sentence counts associated with six South African presidents: Zuma, Mbeki, Ramaphosa, Mandela, Motlanthe, and de Klerk. Zuma leads with 2629 sentences, suggesting he is the most frequently mentioned in the dataset. Mbeki follows closely with 2397 sentences, and Ramaphosa is third with 2279 sentences. Mandela is associated with 1671 sentences, significantly fewer than the top three. Motlanthe and de Klerk are the least mentioned, with only 264 and 97 sentences respectively. Hence given the imbalance of the dataset including Motlanthe and de Klerk, we choose to drop these two presidents from our dataset moving forward.</p>
<div class="cell" data-execution_count="15">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 1: Number of Sentences per President</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Figure 2 illustrates the average sentence lengths for four South African presidents: Mbeki, Mandela, Ramaphosa, and Zuma. Mbeki’s sentences are the longest, averaging around <span class="math inline">\(30.99\)</span> words, indicating more complex or detailed discourse related to him. Mandela’s average is slightly shorter at <span class="math inline">\(25.87\)</span> words per sentence, followed by Ramaphosa with <span class="math inline">\(22.83\)</span> words. Zuma has the shortest sentences, averaging <span class="math inline">\(19.43\)</span> words, suggesting more concise communication associated with him. These differences might reflect variations in communication styles or the nature of the texts related to each president in the dataset.</p>
<div class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 2: Average Sentence Length per President</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In figure 3, the term “government” stands out as the most frequently used, highlighting the focus on governance in the presidential speeches. References to “South,” “Africa,” and economic terms like “development” and “economic” suggest an emphasis on regional affairs and economic growth. Words such as “people,” “public,” and “national” underscore the significance of civic engagement and national identity.</p>
<div class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 3: Top 20 most common words used across all presidents</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>From Figure 4 we can examine the word clouds of South African presidents Mandela, Ramaphosa, Mbeki, and Zuma, noting the distinct thematic priorities that emerge. Mandela’s discourse leaned towards nation-building, emphasizing “government,” “people,” and “national.” Ramaphosa highlighted economic terms like “investment” and “growth,” while maintaining a strong national identity. Mbeki’s governance approach is reflected in terms such as “process” and “system,” with “South African” underscoring unity. Zuma’s tenure was marked by infrastructure and economic themes, seen in “development” and “economy.” Across all leaders, a shared commitment to governance and the South African populace is evident.</p>
<div class="cell" data-execution_count="19">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 4: Word Cloud per President</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="model-evaluation">Model Evaluation</h2>
<section id="accuracy" class="level3">
<h3 class="anchored" data-anchor-id="accuracy">Accuracy</h3>
<p>From the accuracy table (Table 1), it’s evident that the feature extraction method plays a pivotal role in the performance of models when classifying the origin of SONA presidential sentences. When employing the Bag of Words technique, the Support Vector Machines (SVM) model yields the highest accuracy at 59.354%, closely followed by Neural Networks with 58.797% and Boosted Trees (Catboost) at 56.182%. The TF-IDF method sees a similar trend, with SVM leading at 61.471%, Neural Networks registering 59.581%, and Boosted Trees (Catboost) trailing slightly at 55.569%. It’s noteworthy that SVM attains its peak accuracy with the TF-IDF extraction method. However, all three models exhibit a sharp decline in performance when using Word Embeddings, with accuracies plummeting to below 34%. This indicates that, for this particular task, traditional methods like Bag of Words and TF-IDF outperform Word Embeddings. It should be noted that the word embeddings feature extraction despite setting a random state, varied in its performance across different runs, though across all runs still performed the worst of all feature extraction methods investigated in terms of accuracy.</p>
<div class="cell" data-execution_count="21">
<div class="cell-output cell-output-display">

<div>                            <div id="977ba80f-77b1-4ed7-8e40-ea4d786b3d6b" class="plotly-graph-div" style="height:150px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("977ba80f-77b1-4ed7-8e40-ea4d786b3d6b")) {                    Plotly.newPlot(                        "977ba80f-77b1-4ed7-8e40-ea4d786b3d6b",                        [{"cells":{"align":"left","fill":{"color":["lightgrey","white","white","white"]},"font":{"color":"black","family":"Arial, sans-serif","size":11},"height":30,"line":{"color":"darkslategray","width":1},"values":[["Boosted Trees(Catboost)","Support Vector Machines","Neural Networks"],[56.182,59.354,58.797],[55.569,61.471,59.581],[28.902,29.291,33.852]]},"columnwidth":[200,100,100,150],"header":{"align":"left","fill":{"color":"lightgrey"},"font":{"color":"black","family":"Arial, sans-serif","size":12},"height":30,"line":{"color":"darkslategray","width":1},"values":["Models\u002fFeature Extraction","Bag of Words","TF-IDF","Word Embeddings"]},"type":"table"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"margin":{"t":10,"b":0,"l":10,"r":10},"height":150},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('977ba80f-77b1-4ed7-8e40-ea4d786b3d6b');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
<p>Table 1: Accuracy across models and feature extraction methods</p>
</div>
</div>
</section>
<section id="f1-score" class="level3">
<h3 class="anchored" data-anchor-id="f1-score">F1 Score</h3>
<p>The F1-Score table (Table 2) provides insights into the harmonic mean between precision and recall for different models and feature extraction techniques. Using the Bag of Words method, Neural Networks achieve the highest F1-Score at 58.113%, slightly ahead of Support Vector Machines (SVM) which scored 57.850%. The Boosted Trees (Catboost) model lags a bit behind with a score of 53.552%. When the TF-IDF extraction technique is applied, SVM emerges as the top performer with a score of 60.121%. This is followed closely by the Neural Networks model with an F1-Score of 59.113%. Again, Boosted Trees (Catboost) secures the third position, achieving an F1-Score of 53.294%. However, when Word Embeddings are used for feature extraction, there is a notable drop in performance across all models. The Neural Networks model fares the best in this category with an F1-Score of 26.420%, while both SVM and Boosted Trees (Catboost) register scores below 15%. The stark contrast in performance when using Word Embeddings, compared to the other two methods, underscores its limitation for this specific task.</p>
<div class="cell" data-execution_count="22">
<div class="cell-output cell-output-display">

<div>                            <div id="412ee779-0ded-4858-bab2-d3d2960fb9a9" class="plotly-graph-div" style="height:150px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("412ee779-0ded-4858-bab2-d3d2960fb9a9")) {                    Plotly.newPlot(                        "412ee779-0ded-4858-bab2-d3d2960fb9a9",                        [{"cells":{"align":"left","fill":{"color":["lightgrey","white","white","white"]},"font":{"color":"black","family":"Arial, sans-serif","size":11},"height":30,"line":{"color":"darkslategray","width":1},"values":[["Boosted Trees(Catboost)","Support Vector Machines","Neural Networks"],[53.552,57.85,58.113],[53.294,60.121,59.113],[14.47,13.271,26.42]]},"columnwidth":[200,100,100,150],"header":{"align":"left","fill":{"color":"lightgrey"},"font":{"color":"black","family":"Arial, sans-serif","size":12},"height":30,"line":{"color":"darkslategray","width":1},"values":["Models\u002fFeature Extraction","Bag of Words","TF-IDF","Word Embeddings"]},"type":"table"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"margin":{"t":10,"b":0,"l":10,"r":10},"height":150},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('412ee779-0ded-4858-bab2-d3d2960fb9a9');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
<p>Table 2: F1-Score across models and feature extraction methods</p>
</div>
</div>
</section>
</section>
<section id="bag-of-words" class="level2">
<h2 class="anchored" data-anchor-id="bag-of-words">Bag of Words</h2>
<section id="boosted-trees" class="level3">
<h3 class="anchored" data-anchor-id="boosted-trees">Boosted Trees</h3>
<p>Figure 5 shows the mean loss over each 5-fold cross-validation over 300 epochs over the boosted tree. Both losses exhibit a downward trend, signifying improvement in model performance with successive iterations. The training loss demonstrates a consistent decrease, reflecting the model’s enhanced fitting to the training data. Concurrently, the validation loss also descends, suggesting improved model generalization to unseen data. Notably, the training loss decreases at a slightly accelerated rate compared to the validation loss. This observed trend warrants careful monitoring to prevent potential overfitting, ensuring the model maintains its predictive accuracy on external datasets.</p>
<div class="cell" data-execution_count="24">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-25-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 5: Bag of Words Catboost Training and Validation Error Over Epochs</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Figure 6 shows the Boosted Tree’s performance for the categories: Mandela, Mbeki, Ramaphosa, and Zuma. For correct classifications: 93 for Mandela, 278 for Mbeki, 253 for Ramaphosa, and 385 for Zuma, indicating varying degrees of accuracy across classes. Notably, the Boosted Tree’s is most accurate when predicting Zuma and least accurate for Mandela.</p>
<div class="cell" data-execution_count="25">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 6 Bag of Words Catboost Confusion Matrix</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="neural-network" class="level3">
<h3 class="anchored" data-anchor-id="neural-network">Neural Network</h3>
<p>Figure 7 Shows the mean training and validation accuracy across 20 epochs using 5-fold cross-validation on the training dataset. We observe little evidence of significant learning in the validation set over the epochs, with the validation accuracy remaining constant over the 20 epochs. Hence given this the final model selected was chosen to run on 2 epochs. A possible reason for this could be the choice of the kernel weight initializer which provides a non-naive initial set of weights for the NN.</p>
<div class="cell" data-execution_count="27">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure: 7 Bag of Words Neural Network Average Training and Validation Accuracy Over Epochs</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In Figure 8 Neural Network classifier exhibits varying degrees of classification accuracy across different classes. For Mandela, out of 334 instances, 170 were correctly classified, with an accuracy of 50.9%. The Mbeki class had 264 (55.0%) a. The classifier achieved higher accuracy for Ramaphosa, correctly classifying 291 (63.8%). The highest accuracy was observed for the Zuma class, with 331 (62.9%).</p>
<p>With respect to the misclassifications: Mandela was often misclassified as Mbeki (92 instances) or Zuma (45 instances). Mbeki instances were misclassified as Mandela (91 instances) and Zuma (76 instances). There were 97 Ramaphosa instances misclassified as Zuma. For the Zuma class, there were 88, 52, and 55 instances misclassified as Ramaphosa, Mandela, and Mbeki respectively.</p>
<div class="cell" data-execution_count="30">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-31-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 8: Bag of Words Neural Network Confusion Matrix</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="svms" class="level3">
<h3 class="anchored" data-anchor-id="svms">SVMs</h3>
<p>Figure 9 shows for the Mandela class, there were 139(41.6%). The Mbeki class had an accuracy of 54.8%. The classifier was more accurate in identifying Ramaphosa, correctly classifying 271 (59.4%). The highest classification accuracy was observed for the Zuma class, with 393 correct, achieving 74.7% accuracy.</p>
<p>Patterns of misclassification were also observed across the classes. For instance, Mandela was often misclassified as Mbeki (80 instances) and Zuma (77 instances). Instances of Mbeki were misclassified as Mandela (71 instances) and Zuma (99 instances). Notably, Ramaphosa was misclassified as Zuma in 126 instances.</p>
<div class="cell" data-execution_count="33">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-34-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 9: Bag of Words SVM Confusion Matrix</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="tf-idf" class="level2">
<h2 class="anchored" data-anchor-id="tf-idf">TF-IDF</h2>
<section id="boosted-trees-1" class="level3">
<h3 class="anchored" data-anchor-id="boosted-trees-1">Boosted Trees</h3>
<p>The results of the training over epochs for the GBT model using TF-IDF can be seen in the appendix(Figure A1), though are largely the same in behaviour as the GBT model trained using the BoW feature extraction. Hence the final model chosen has the same hyperparameters. The confusion matrix for the Gradient Boosting Trees (GBT) classifier (Figure 10) reveals a classification accuracy of 28.7% for Mandela, with 96 correct predictions. The Mbeki class demonstrated a higher accuracy of 59.2% with 284 correct predictions from 480 instances. The Ramaphosa class was accurately identified in 261 out of 456 instances, yielding an accuracy of 57.2%. The Zuma class exhibited the highest classification accuracy at 68.3%, with 359 correct predictions from 526 instances. In terms of misclassifications, these appear less pronounced but we do see, that Mandela was often confused with Mbeki (113 instances).</p>
<div class="cell" data-execution_count="36">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-37-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 10: TF-IDF Catboost Confusion Matrix</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="svm" class="level3">
<h3 class="anchored" data-anchor-id="svm">SVM</h3>
<div class="cell" data-execution_count="39">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-40-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 11: TF-IDF SVM Confusion Matrix</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Looking to Figure 11 for the SVM classifier using TF- IDF feature extraction the confusion matrix denotes: Mandela at 42.8%, with 143; Mbeki at 61.9%, with 297 correct predictions; Ramaphosa at 64.3%, with 293 correct predictions; and Zuma at 70.5%, with 371 correct predictions.</p>
<p>In terms of notable misclassifications, the classifier misidentified Ramaphosa 109 times as Zuma.</p>
</section>
<section id="neural-net" class="level3">
<h3 class="anchored" data-anchor-id="neural-net">Neural Net</h3>
<p>Figure 12 illustrates the confusion matrix for the NN trained using TF-IDF feature extraction. For Mandela, the NN secured 182 correct predictions (54.5%). The Mbeki class observed an accuracy of 60.6% with 291 correct predictions. For Ramaphosa, 276 correct predictions were made (60.5%). The highest accuracy was noted for the Zuma class, with 321 with an accuracy of 61.0%.</p>
<p>In terms of misclassifications, we note similar patterns in misclassifications between Zuma and Ramaphosa.</p>
<div class="cell" data-execution_count="43">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-44-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 12: TF-IDF Neural Network Confusion Matrix</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="word-embeddings-1" class="level2">
<h2 class="anchored" data-anchor-id="word-embeddings-1">Word Embeddings</h2>
<section id="catboost" class="level3">
<h3 class="anchored" data-anchor-id="catboost">Catboost</h3>
<p>Figure 13 shows confusion matrix of the GBT using word embeddings. For the Mandela class, out of 334 instances, there were no correct predictions, resulting in a classification accuracy of 0%. The Mbeki class similarly did not have any correct predictions, but 3 instances were misclassified as Mandela. The Ramaphosa class achieved the highest accuracy, with 505 correct predictions, translating to an accuracy of 96%. The Zuma class had 14 correct predictions, yielding an accuracy of 3.1%.</p>
<p>The significant misclassifications into the Ramaphosa class from other categories suggest a strong bias in the classifier towards this class, which is concerning yet somewhat unsurprising given the model’s performance in terms of accuracy and F1-score.</p>
<div class="cell" data-execution_count="47">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-48-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 13: Word Embeddings Catboost Confusion Matrix</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="svm-1" class="level3">
<h3 class="anchored" data-anchor-id="svm-1">SVM</h3>
<p>In Figure 14 we see the word embeddings SVM confusion matrix: The model upon investigation here has failed greatly. It has simply predicted all sentences belonging to Ramaphosa and hence cannot be interpreted at any meaningful level.</p>
<div class="cell" data-execution_count="51">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 14: Word Embeddings SVM Confusion Matrix</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="neural-network-1" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-1">Neural Network</h3>
<p>We finally examine the confusion matrix for the NN using word embeddings in Figure 15. The Neural Network classifier’s performance varied across the classes. In Mandela’s class, none were correctly predicted, with 235 instances being misclassified as Ramaphosa. The Mbeki class had a somewhat better accuracy of 28.3%, securing 136, but a notable 323 instances were erroneously labelled as Ramaphosa. The classifier’s strongest performance was with the Ramaphosa class, achieving an accuracy of 81.9%, correctly predicting 431 instances. The Zuma class resulted in an accuracy of 9.0%, with 41 accurate predictions from 456 instances. However, a significant number, 347 instances, were mislabeled as Ramaphosa.</p>
<div class="cell" data-execution_count="55">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-56-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 15: Word Emdeddings Neural Network Confusion Matrix</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>The findings of this study emphasize the pivotal role of feature extraction methods in classifying sentences from SONA speeches of various South African presidents. Of the three feature extraction methods, traditional methods (BoW and TF-IDF) consistently outperformed Word Embeddings across the predictive models used.</p>
<p>The superior performance of SVMs, especially when paired with TF-IDF features, is noteworthy. This could be attributed to SVM’s inherent ability to efficiently handle high-dimensional data spaces and its adeptness at discerning intricate patterns from the SONA speeches. Such capability highlights SVM’s robustness, especially when dealing with text data that may have nuances in vocabulary and structure.</p>
<p>An observation from the data exploration phase was the imbalance present in the dataset. Zuma’s significant representation, contrasted with the sparse data associated with figures like de Klerk and Montlanthe, was an evident challenge. These imbalances can lead models to be biased towards overrepresented classes. This bias was manifested in the Gradient Boosted Trees model’s strong inclination towards the Ramaphosa class when utilizing Word Embeddings. Addressing this imbalance in future studies, perhaps through techniques such as oversampling or the generation of synthetic data, may yield more balanced and generalizable results.</p>
<p>The variations in average sentence lengths across different presidents also offered intriguing insights. For instance, Mbeki’s inclination for longer sentences could suggest a more detailed or intricate discourse style. Such differences might be indicative of unique communication styles, rhetorical strategies, or focal points.</p>
<p>The consistent decline in performance across all models when utilizing Word Embeddings was a surprising outcome. One potential explanation could be that the embeddings, especially if pre-trained on generic corpora, may not be adept at capturing the linguistic nuances of SONA speeches. This underscores the importance of domain-specific training or fine-tuning when dealing with specialized datasets.</p>
<p>The results from the Neural Networks, while promising, did not significantly overshadow the simpler SVM model. Given the depth and potential of Neural Networks, it might be beneficial to explore more exhaustive hyperparameter tuning or even delve into more advanced architectures. Sequence models, such as Recurrent Neural Networks (RNNs), which have demonstrated prowess in capturing the sequential nature of linguistic data, might be particularly promising for future iterations of this study. The computational efficiency of the SVM especially when compared to NNs, GBTS, and transformer transfer learning models like BERT underscores the value and impressive performance of SVMs in NLP classification tasks.</p>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>In summary, while the study has successfully delineated the linguistic tendencies across various South African presidents, it has also highlighted challenges and potential avenues for refinement. The interplay of feature extraction methods, model selection, and data intricacies presents a complex landscape, emphasizing the multifaceted nature of political speech classification. Future research can build upon these findings, integrating more advanced models, addressing dataset imbalances, and potentially fusing insights from other academic disciplines like linguistics and political science for a holistic understanding. Though, this paper has succeeded in critically examing the landscape of NLP classification, providing a robust direction for work in political NLP classification and detailed analysis and evaluation of the methods available.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>